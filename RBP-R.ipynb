{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "chronic-establishment",
   "metadata": {},
   "source": [
    "# Phage-host predictor: RBP-R\n",
    "\n",
    "This is the main notebook for the Klebsiella phage-host interaction prediction based on phage RBPs and Klebsiella's K-locus. The idea is to start with something simple and add on to it iteratively.\n",
    "\n",
    "The first simple framework is as follows:\n",
    "1. Identify the K locus on the bacterial genome using Kaptive\n",
    "2. Transform the locus into an embedding using Word2Vec\n",
    "3. Transform the RBP sequence into an embedding using UniRep, ProtTransBert, SeqVec, ...\n",
    "4. Construct reliable negative interactions by random sampling with constraints that make sense (use RBPs and loci to create pairwise alignment scores)\n",
    "5. Concatenate and train an RF model via cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-brush",
   "metadata": {},
   "source": [
    "## 0 - Libraries and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "drawn-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "from Bio.Blast import NCBIWWW, NCBIXML\n",
    "from Bio.Blast.Applications import NcbiblastpCommandline\n",
    "from bio_embeddings.embed import Word2VecEmbedder, ProtTransBertBFDEmbedder, SeqVecEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "together-constitution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set project directory (to locate Kaptive), data directory and results directory\n",
    "project_dir = '/Users/dimi/Documents/GitHub_Local/PhageHostLearning'\n",
    "data_dir = '/Users/dimi/GoogleDrive/PhD/4_PHAGEHOST_LEARNING/42_DATA/Klebsiella_RBP_data'\n",
    "results_dir = '/Users/dimi/GoogleDrive/PhD/4_PHAGEHOST_LEARNING/43_RESULTS'\n",
    "cdpath = '/opt/anaconda3/pkgs/cd-hit-4.8.1-hd9629dc_0/bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-wagner",
   "metadata": {},
   "source": [
    "## 1 - Identify the K locus using Kaptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "julian-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaptive_python(project_dir, data_dir, database_name, file_name):\n",
    "    \"\"\"\n",
    "    This function is a wrapper for the Kaptive Python file to be executed from another Python script.\n",
    "    This wrapper runs on a single FASTA file (one genome) and also produces a single FASTA file.\n",
    "    \n",
    "    Input:\n",
    "    - project_directory: the location of kaptive.py (preferrably in the same project folder)\n",
    "    - data_directory: location of the database and sequence file(s) to loop over\n",
    "    - database_name: string of the name of the database (.gbk file)\n",
    "    - file_name: string of the file name (FASTA)\n",
    "    \n",
    "    Output:\n",
    "    - a single fasta file of the locus (single piece or multiple ones) per genome\n",
    "    \"\"\"\n",
    "    cd_command = 'cd ' + project_dir\n",
    "    \n",
    "    kaptive_command = 'python kaptive.py -a ' + data_dir + '/' + file_name + ' -k ' + data_dir + '/' + database_name + ' -o ' + data_dir + '/ --no_table --no_json'\n",
    "    command = cd_command + '; ' + kaptive_command\n",
    "    ssprocess = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    ssout, sserr = ssprocess.communicate()\n",
    "    kaptive_file_name = 'kaptive_results_'+file_name\n",
    "    \n",
    "    return kaptive_file_name, ssout, sserr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "awful-sally",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loci(klebsiella_genomes, project_dir, data_dir, database_name):\n",
    "    \"\"\"\n",
    "    This function uses kaptive_python to loop over all klebsiella_genomes, construct FASTA files for\n",
    "    each of them and identify their loci. Importantly, the file_names are later used to construct embeddings,\n",
    "    so need to be identifiable (accession numbers).\n",
    "    \n",
    "    EITHER work with phagebase_1 genomes, or with RBPbase accession numbers, what's best?\n",
    "    \n",
    "    Input:\n",
    "    - klebsiella_genomes: Pandas DataFrame of genomes w/ following column names:\n",
    "        'accession', 'sequence' and 'number_of_prophages'\n",
    "    - project_directory: the location of kaptive.py (preferrably in the same project folder)\n",
    "    - data_directory: location of the database and sequence file(s) to loop over\n",
    "    - results_directory: location to store results\n",
    "    - database_name: string of the name of the database (.gbk file)\n",
    "    \"\"\"\n",
    "    kaptive_file_names = []\n",
    "    pbar = tqdm(total=klebsiella_genomes.shape[0])\n",
    "    \n",
    "    # loop over klebsiella_genomes\n",
    "    for i, genome in enumerate(klebsiella_genomes['sequence']):\n",
    "        #if klebsiella_genomes['number_of_prophages'][i] > 0: # no filter: rows/columns consistent down the road\n",
    "        acc = klebsiella_genomes['accession'][i]\n",
    "\n",
    "        # make FASTA file\n",
    "        file_name = acc+'.fasta'\n",
    "        fasta = open(data_dir+'/'+file_name, 'w')\n",
    "        fasta.write('>'+acc+'\\n'+genome+'\\n')\n",
    "        fasta.close()\n",
    "\n",
    "        # run Kaptive\n",
    "        kaptive_file, _, _ = kaptive_python(project_dir, data_dir, database_name, file_name)\n",
    "        kaptive_file_names.append(kaptive_file)\n",
    "\n",
    "        # delete FASTA file\n",
    "        os.remove(data_dir+'/'+file_name)\n",
    "            \n",
    "        # update progress\n",
    "        pbar.update(1)\n",
    "            \n",
    "    pbar.close()\n",
    "    return kaptive_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "innovative-cowboy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 376/376 [13:00:28<00:00, 124.54s/it]   \n"
     ]
    }
   ],
   "source": [
    "database = 'Klebsiella_k_locus_primary_reference.gbk'\n",
    "genomes = pd.read_csv(data_dir+'/phagebase1_klebsiella_pneumoniae.csv')\n",
    "names = compute_loci(genomes, project_dir, data_dir, database) # results in the same data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-continuity",
   "metadata": {},
   "source": [
    "## 2 - Transform locus into embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "herbal-madness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def locus_embeddings(kaptive_file_names, data_dir):\n",
    "    \"\"\"\n",
    "    This functions computes Word2Vec embeddings for bacterial genomic loci. It uses the bio-embeddings\n",
    "    package together with BioPython to transform fasta sequences of bacterial loci into numerical embeddings.\n",
    "    \n",
    "    WARNING: running this on a personal laptop without GPU can be slow (not advised). Best to start a Google\n",
    "    Colab notebook and run the computations in the cloud.\n",
    "    \n",
    "    Input:\n",
    "    - kaptive_file_names: list of fasta filenames of bacterial loci (output of Kaptive)\n",
    "    - data_dir: location of the sequence file(s) to loop over\n",
    "    - results_dir: directory to store the resulting embeddings in.\n",
    "    \n",
    "    Output:\n",
    "    - embeddings: Pandas DataFrame (as .csv file) with file names and their corresponding embedding\n",
    "    \"\"\"\n",
    "    embedder = Word2VecEmbedder()\n",
    "    embeddings_list = []\n",
    "\n",
    "    for name in tqdm(kaptive_file_names):\n",
    "        locus_sequence = ''\n",
    "        # one or multiple records in fasta file\n",
    "        for record in SeqIO.parse(data_dir+'/'+name, 'fasta'):\n",
    "            locus_sequence += str(record.seq)\n",
    "        \n",
    "        # compute embedding\n",
    "        embedding = embedder.embed(locus_sequence)\n",
    "        reduced_embedding = embedder.reduce_per_protein(embedding)\n",
    "        embeddings_list.append(reduced_embedding)\n",
    "        \n",
    "    embeddings_df = pd.concat([pd.DataFrame(kaptive_file_names), pd.DataFrame(embeddings_list)], axis=1)\n",
    "    embeddings_df.to_csv(data_dir+'/locus_embeddings.csv', index=False)\n",
    "    \n",
    "    return      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acting-lincoln",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 376/376 [00:29<00:00, 12.69it/s]\n"
     ]
    }
   ],
   "source": [
    "locus_embeddings(names, data_dir, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-spencer",
   "metadata": {},
   "source": [
    "## 3 - Tranform RBP sequences into embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "provincial-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RBPbase_identifiers(rbp_data, data_dir):\n",
    "    \"\"\"\n",
    "    This function adds unique IDs to all RBP sequences in the RBP database. As the unit for further\n",
    "    processing is an RBP sequence, each RBP sequence should have a unique identifier.\n",
    "\n",
    "    Input:\n",
    "    - rbp_data: name of the RBP database (string)\n",
    "    - data_dir: location of the database and where the fasta files will be stored (string)\n",
    "\n",
    "    Output:\n",
    "    - adjusted RBPbase (extra column with unique_ID)\n",
    "    \"\"\"\n",
    "    if data_dir != '':\n",
    "        data_dir = data_dir+'/'\n",
    "\n",
    "    RBPbase = pd.read_csv(data_dir+rbp_data)\n",
    "    unique_id_list = []\n",
    "    phage_id_list = []\n",
    "    for i, phage_nr in enumerate(RBPbase['phage_nr']):\n",
    "        phage_id = str(phage_nr)+str(RBPbase['host'][i])\n",
    "        # first RBP\n",
    "        if phage_id not in phage_id_list:\n",
    "            count = 0\n",
    "            phage_id_list.append(phage_id)\n",
    "            unique_id_list.append(phage_id+'_RBP'+str(count))\n",
    "        # second, third, ... RBP\n",
    "        else:\n",
    "            count += 1\n",
    "            unique_id_list.append(phage_id+'_RBP'+str(count))\n",
    "\n",
    "    RBPbase['unique_ID'] = unique_id_list\n",
    "    RBPbase.to_csv(data_dir+rbp_data)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "faced-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RBPbase_species_filter(rbp_data, data_dir, species):\n",
    "    \"\"\"\n",
    "    This function creates a subset of RBPbase for a specific host species for further processing.\n",
    "\n",
    "    Input:\n",
    "    - rbp_data: name of the RBP database (string)\n",
    "    - data_dir: location of the database and where the fasta files will be stored (string)\n",
    "    - species: the single species to create a subset for\n",
    "\n",
    "    Output:\n",
    "    - subset RBPbase\n",
    "    \"\"\"\n",
    "    if data_dir != '':\n",
    "        data_dir = data_dir+'/'\n",
    "    RBPbase = pd.read_csv(data_dir+rbp_data)\n",
    "    to_delete = [i for i, host in enumerate(RBPbase['host']) if host != species]\n",
    "    RBPbase = RBPbase.drop(to_delete, axis=0)\n",
    "    RBPbase = RBPbase.reset_index(drop=True)\n",
    "    filepieces = rbp_data.split('.')\n",
    "    RBPbase.to_csv(data_dir+filepieces[0]+'_'+species+'.'+filepieces[1], index=False)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "satisfactory-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RBPbase_fasta_processing(rbp_data, data_dir):\n",
    "    \"\"\"\n",
    "    This function processes the RBP database from a Pandas DataFrame to individual fasta files that can be\n",
    "    looped over to compute protein embeddings.\n",
    "\n",
    "    Input:\n",
    "    - rbp_data: name of the RBP DataFrame (string) in the data_dir, with protein sequences in column 'protein_seq'\n",
    "    - data_dir: location of the database and where the fasta files will be stored (string)\n",
    "\n",
    "    Output:\n",
    "    - fasta files of each of the RBP sequences in the database\n",
    "    - big fasta file of all sequences together (for pairwise alignments later)\n",
    "    \"\"\"\n",
    "    if data_dir != '':\n",
    "        data_dir = data_dir+'/'\n",
    "    if results_dir != '':\n",
    "        results_dir = results_dir+'/'\n",
    "\n",
    "    rbp_file_names = []\n",
    "    RBPbase = pd.read_csv(data_dir+rbp_data)\n",
    "    big_fasta = open(data_dir+rbp_data.split('.')[0]+'.fasta', 'w')\n",
    "    for i, sequence in enumerate(RBPbase['protein_seq']):\n",
    "        unique_id = RBPbase['unique_ID'][i]\n",
    "        rbp_file_names.append(unique_id+'.fasta')\n",
    "        \n",
    "        # write individual fasta\n",
    "        fasta = open(data_dir+unique_id+'.fasta', 'w')\n",
    "        fasta.write('>'+unique_id+'\\n'+sequence+'\\n')\n",
    "        fasta.close()\n",
    "        \n",
    "        # write big fasta\n",
    "        big_fasta.write('>'+unique_id+'\\n'+sequence+'\\n')\n",
    "    big_fasta.close()\n",
    "\n",
    "    return rbp_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "anticipated-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "def protein_embeddings(rbp_file_names, data_dir):\n",
    "    \"\"\"\n",
    "    This functions computes ProttransBERT embeddings for phage RBPs. It uses the bio-embeddings\n",
    "    package together with BioPython to transform fasta sequences of phage RBPs into numerical embeddings.\n",
    "    \n",
    "    WARNING: running this on a personal laptop without GPU is very slow and not advised. Best to start a Google\n",
    "    Colab or Kaggle notebook and run the computations in the cloud.\n",
    "    \n",
    "    Input:\n",
    "    - rbp_file_names: list of fasta filenames of phage RBPs\n",
    "    - data_dir: location of the sequence file(s) to loop over\n",
    "    - results_dir: directory to store the resulting embeddings in\n",
    "    \n",
    "    Output:\n",
    "    - embeddings: Pandas DataFrame (as .csv file) with file names and their corresponding embedding\n",
    "    \"\"\"\n",
    "    embedder = ProtTransBertBFDEmbedder()\n",
    "    embeddings_list = []\n",
    "    \n",
    "    if data_dir != '':\n",
    "        data_dir = data_dir+'/'\n",
    "\n",
    "    for name in tqdm(rbp_file_names):\n",
    "        for record in SeqIO.parse(data_dir+name, 'fasta'):\n",
    "            rbp_sequence = str(record.seq)\n",
    "        \n",
    "        embedding = embedder.embed(rbp_sequence)\n",
    "        reduced_embedding = embedder.reduce_per_protein(embedding)\n",
    "        embeddings_list.append(reduced_embedding)\n",
    "        \n",
    "    embeddings_df = pd.concat([pd.DataFrame(rbp_file_names), pd.DataFrame(embeddings_list)], axis=1)\n",
    "    embeddings_df.to_csv(data_dir+'protein_embeddings.csv', index=False)\n",
    "    \n",
    "    return  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "spanish-repository",
   "metadata": {},
   "outputs": [],
   "source": [
    "RBPbase_identifiers('RBPbase_250621.csv', data_dir)\n",
    "RBPbase_species_filter('RBPbase_250621.csv', data_dir, 'klebsiella_pneumoniae')\n",
    "rbp_file_names = RBPbase_fasta_processing('RBPbase_250621_klebsiella_pneumoniae.csv', data_dir)\n",
    "pd.DataFrame(rbp_file_names, columns=['names']).to_csv(data_dir+'/rbp_file_names.csv', index=False)\n",
    "#protein_embeddings(rbp_file_names, data_dir) # do in Google Colab or Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-scholar",
   "metadata": {},
   "source": [
    "## 4 - Construct reliable negative interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "postal-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kaptive_file_names(klebsiella_genomes):\n",
    "    \"\"\"\n",
    "    This function is a support function to recollect all kaptive file names without\n",
    "    having to recompute all loci.\n",
    "    \n",
    "    Input: klebsiella genomes DataFrame (columns: 'accession', 'sequence' and 'number_of_prophages')\n",
    "    Output: list kaptive_file_names\n",
    "    \"\"\"\n",
    "    kaptive_file_names = []\n",
    "    for acc in klebsiella_genomes['accession']:\n",
    "        file_name = 'kaptive_results_'+acc+'.fasta'\n",
    "        kaptive_file_names.append(file_name)\n",
    "    return kaptive_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "random-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaptive_fasta_processing(kaptive_file_names, data_dir):\n",
    "    \"\"\"\n",
    "    This function processes all the separate bacterial loci FASTA files into one merged FASTA file for \n",
    "    further processing (pairwise alignments).\n",
    "    \n",
    "    Input:\n",
    "    - kaptive_file_names: list of fasta filenames of bacterial loci (output of Kaptive)\n",
    "    - data_dir: location of the sequence file(s) to loop over\n",
    "    \"\"\"\n",
    "    kaptive_fasta_all = data_dir+'/kaptive_results_all.fasta'\n",
    "    big_fasta = open(kaptive_fasta_all, 'w')\n",
    "    for name in kaptive_file_names:\n",
    "        kaptive_id = name.split('.fasta')[0].split('kaptive_results_')[1]\n",
    "        locus_sequence = ''\n",
    "        for record in SeqIO.parse(data_dir+'/'+name, 'fasta'):\n",
    "            locus_sequence += str(record.seq)   \n",
    "        big_fasta.write('>'+kaptive_id+'\\n'+locus_sequence+'\\n')\n",
    "    big_fasta.close()\n",
    "\n",
    "    return kaptive_fasta_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "nearby-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_alignment_julia(file_name, align_type, project_dir, n_threads='4'):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - file_name: string of path to the FASTA file to loop over\n",
    "    - align_type: type of alignment to execute ('DNA' or 'protein')\n",
    "    - project_dir: project directory with julia file in it\n",
    "    - n_threads: number of threads to use for multithreading (as string; default=4)\n",
    "    \n",
    "    Output:\n",
    "    - a score matrix of pairwise ID%, named file_name + '_score_matrix.txt'\n",
    "    \"\"\"\n",
    "    threads_command = 'export JULIA_NUM_THREADS=' + n_threads\n",
    "    alias_command = 'alias julia=\"/Applications/Julia-1.5.app/Contents/Resources/julia/bin/julia\"'\n",
    "    cd_command = 'cd ' + project_dir\n",
    "    pw_command = 'julia pairwise_alignment.jl ' + file_name + ' ' + align_type\n",
    "    \n",
    "    command = threads_command + '; ' + alias_command + '; ' + cd_command + '; ' + pw_command\n",
    "    ssprocess = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    ssout, sserr = ssprocess.communicate()\n",
    "    \n",
    "    return ssout, sserr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "moving-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdhit_est_python(cdhit_path, kaptive_file_names, kaptive_fasta, output_file, c=0.90, n=7):\n",
    "    \"\"\"\n",
    "    This function executes CD-HIT-EST (DNA sequences) clustering commands from within Python. \n",
    "    To install CD-HIT, do so via conda: conda install -c bioconda cd-hit.\n",
    "    \n",
    "    Input:\n",
    "        - cdhit_path: path to CD-HIT software\n",
    "        - kaptive_file_names: list of file names for ordering\n",
    "        - kaptive_fasta: FASTA file with locus sequences\n",
    "        - output file: path to output (will be one FASTA file and one .clstr file)\n",
    "        - c: threshold on identity for clustering\n",
    "        - n: word length (7 for thresholds between 0.88 and 0.9, DNA level)\n",
    "    \"\"\"\n",
    "    \n",
    "    # perform clustering\n",
    "    cd_command = 'cd ' + cdhit_path\n",
    "    cluster_command = 'cd-hit-est -i ' + kaptive_fasta + ' -o ' + output_file + ' -c ' + str(c) + ' -n ' + str(n) + ' -d 0'\n",
    "    command = cd_command + '; ' + cluster_command\n",
    "    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    stdout, stderr = process.communicate()\n",
    "    \n",
    "    # load results and process\n",
    "    score_matrix = np.zeros((len(kaptive_file_names), len(kaptive_file_names)))\n",
    "    clusters = open(output_file+'.clstr')\n",
    "    cluster_iter = 0\n",
    "    cluster_accessions = []\n",
    "    for line in clusters.readlines():\n",
    "        # new cluster\n",
    "        if line[0] == '>':\n",
    "            # finish old cluster if not first one\n",
    "            if (cluster_iter > 0) and (len(cluster_accessions) > 1):\n",
    "                indices = [kaptive_file_names.index('kaptive_results_'+acc+'.fasta') for acc in cluster_accessions]\n",
    "                for i in range(len(indices)-1):\n",
    "                    for j in range(i, len(indices)):\n",
    "                        if indices[i] != indices[j]:\n",
    "                            score_matrix[indices[i],indices[j]], score_matrix[indices[j],indices[i]] = c, c\n",
    "                \n",
    "            # initiate new cluster\n",
    "            cluster_accessions = []\n",
    "            cluster_iter += 1\n",
    "            \n",
    "        # in a cluster\n",
    "        else:\n",
    "            acc = line.split('>')[1].split('...')[0]\n",
    "            cluster_accessions.append(acc)\n",
    "            \n",
    "    # finish last cluster\n",
    "    if len(cluster_accessions) > 1:\n",
    "        indices = [kaptive_file_names.index('kaptive_results_'+acc+'.fasta') for acc in cluster_accessions]\n",
    "        for i in range(len(indices)-1):\n",
    "            for j in range(i, len(indices)):\n",
    "                score_matrix[i,j], score_matrix[j,i] = c, c\n",
    "    \n",
    "    np.fill_diagonal(score_matrix, 1)\n",
    "    np.savetxt(output_file+'_score_matrix.txt', score_matrix, fmt='%.3f')\n",
    "            \n",
    "    return stdout, stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process kaptive loci\n",
    "genomes = pd.read_csv(data_dir+'/phagebase1_klebsiella_pneumoniae.csv')\n",
    "kaptive_file_names = get_kaptive_file_names(genomes)\n",
    "kaptive_fasta_all = kaptive_fasta_processing(kaptive_file_names, data_dir)\n",
    "kaptive_out = data_dir+'/kaptive_loci'\n",
    "_, _ = cdhit_est_python(cdpath, kaptive_file_names, kaptive_fasta_all, kaptive_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process RBP sequences\n",
    "\"\"\"\n",
    "cmd line: julia pairwise_alignment.jl \"/Users/Dimi/GoogleDrive/PhD/4_PHAGEHOST_LEARNING/42_DATA/Klebsiella_RBP_data/RBPbase_250621_klebsiella_pneumoniae.fasta\" \"protein\"\n",
    "takes about 30 mins\n",
    "\"\"\"\n",
    "pairwise_alignment_julia(data_dir+'/RBPbase_250621_klebsiella_pneumoniae.fasta', 'protein', project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "instructional-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrained_random_interaction_matrix(loci_similarities, loci_threshold, rbp_similarities, rbp_threshold,\n",
    "                                          kaptive_file_names, rbp_file_names, rbp_data):\n",
    "    \"\"\"\n",
    "    This function constructs an interaction matrix for training by adding known positive interactions\n",
    "    and using similarity scores to sample negative interactions in a random manner, with constraints.\n",
    "    \n",
    "    Input:\n",
    "    - loci_similarities, rbp_similarities: similarity score matrices (hosts m x m and phages n x n)\n",
    "    - loci_threshold, rbp_threshold: thresholds above which not to consider negative interactions\n",
    "    - kaptive_file_names, rbp_file_names: lists of names to construct known interactions\n",
    "    - rbp_data: name of the RBP DataFrame (string) (columns: 'host_accession', 'unique_ID')\n",
    "    \n",
    "    Output:\n",
    "    - interaction matrix\n",
    "    \"\"\"\n",
    "    interaction_matrix = np.empty((len(kaptive_file_names), len(rbp_file_names)))\n",
    "    interaction_matrix[:] = np.nan\n",
    "\n",
    "    # fill in known interactions\n",
    "    for i, host_acc in enumerate(rbp_data['host_accession']):\n",
    "        rbp_id = rbp_data['unique_ID'][i]\n",
    "        index_m = kaptive_file_names.index('kaptive_results_'+host_acc+'.fasta')\n",
    "        index_n = rbp_file_names.index(rbp_id+'.fasta')\n",
    "        interaction_matrix[index_m, index_n] = 1\n",
    "    print('check? ', i == len(rbp_file_names))\n",
    "    \n",
    "    # sample random RBP and bacterial locus\n",
    "    iteration = 0\n",
    "    while iteration < len(rbp_file_names):\n",
    "        random_rbp = random.choice(range(len(rbp_file_names)))\n",
    "        random_locus = random.choice(range(len(kaptive_file_names)))\n",
    "\n",
    "        # construct reliable negative interaction within constraints\n",
    "        if interaction_matrix[random_locus, random_rbp] != 1:\n",
    "            # collect similar RBPs and their interacting loci\n",
    "            rbp_similarities_row = rbp_similarities[random_rbp,:]\n",
    "            rbp_threshold_indices = [index for index, sim in enumerate(rbp_similarities_row) if sim > rbp_threshold]\n",
    "            loci_threshold_indices = [index for thr_index in threshold_indices for index in range(interaction_matrix.shape[0]) \n",
    "                                      if interaction_matrix[index, thr_index] == 1]\n",
    "            duplicates = [dup for dup in loci_threshold_indices if loci_threshold_indices.count(dup) > 1]\n",
    "\n",
    "            # assess similarity between collected loci\n",
    "            loci_similarities_row = loci_similarities[random_locus,:]\n",
    "            loci_exceeding_threshold = [index for index in loci_threshold_indices if loci_similarities_row[index] \n",
    "                                        > loci_threshold]\n",
    "            if len(loci_exceeding_threshold) == 0:\n",
    "                interaction_matrix[random_locus, random_rbp] = 0\n",
    "                iteration += 1\n",
    "    return interaction_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "seeing-norfolk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check?  False\n"
     ]
    }
   ],
   "source": [
    "loci_sim = np.loadtxt(data_dir+'/kaptive_loci_score_matrix.txt')\n",
    "loci_thr = 0.9\n",
    "rbp_sim = np.loadtxt(data_dir+'/RBPbase_250621_klebsiella_pneumoniae.fasta_score_matrix.txt')\n",
    "rbp_thr = 0.9\n",
    "rbp_file_names = list(pd.read_csv(data_dir+'/rbp_file_names.csv')['names'])\n",
    "rbpbase = pd.read_csv(data_dir+'/RBPbase_250621_klebsiella_pneumoniae.csv')\n",
    "interaction_matrix = constrained_random_interaction_matrix(loci_sim, loci_thr, rbp_sim, rbp_thr, kaptive_file_names, \n",
    "                                                         rbp_file_names, rbpbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "editorial-amateur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        nan, nan, nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan]])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction_matrix[:5,:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "changed-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbpbase = pd.read_csv(data_dir+'/RBPbase_250621_klebsiella_pneumoniae.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "assigned-contribution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['phage_nr', 'host', 'host_accession', 'sequence', 'protein_seq',\n",
       "       'N_blocks', 'C_blocks', 'N_ranges', 'C_ranges', 'unique_ID'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbpbase.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-offer",
   "metadata": {},
   "source": [
    "## 5 - Processing into structured dataframe for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "parallel-meter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(int_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "simple-broadcasting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_matrix = np.array([[1, 0, 0, 1], [0, math.nan, math.nan, 0], [0, 1, math.nan, 0]])\n",
    "[index for thr_index in [1, 2, 3] for index in range(3) if int_matrix[index, thr_index] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "selective-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp_em_mini = pd.read_csv(data_direct+'/protein_embeddings.csv', index_col=0).iloc[:4,:]\n",
    "int_matrix = np.array([[1, 0, 0, 1], [0, math.nan, math.nan, 0], [0, 1, math.nan, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "classical-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_training_dataframe(locus_features, rbp_features, interaction_matrix, RBPbase=0):\n",
    "    \"\"\"\n",
    "    This function combines the seraparetly computed features of both the locus and the RBPs\n",
    "    into one array suitable for training, considering the interaction matrix.\n",
    "    \n",
    "    At this point, the rows of the featuresets should correspond with the rows and columns of the interaction\n",
    "    matrix, meaning the interaction matrix should be constructed starting from the same data as the features!\n",
    "    \n",
    "    Input:\n",
    "    - locus features: DataFrame of the features constructed for the bacterial loci (m x #feats)\n",
    "    - rbp_features: DataFrame of the features constructured for the phage RBPs (n x #feats)\n",
    "    - interaction matrix: (m x n) with positives (1), negatives (0) and unknown (NaN)\n",
    "    - RBPbase: RBP dataframe for sanity check (optional)\n",
    "    \"\"\"\n",
    "    \n",
    "    # define lists to append to\n",
    "    locus_list = []; rbp_list = []; interaction_list = []\n",
    "    \n",
    "    # loop over interaction matrix and construct dataframe\n",
    "    for i in range(interaction_matrix.shape[0]):\n",
    "        for j in range(interaction_matrix.shape[1]):\n",
    "            interaction = interaction_matrix[i,j]\n",
    "            \n",
    "            if math.isnan(interaction) == False:\n",
    "                # make list of lists, that can subsequently be combined\n",
    "                locus_list.append(list(locus_features.iloc[i,:]))\n",
    "                rbp_list.append(list(rbp_features.iloc[j,:]))\n",
    "                interaction_list.append(int(interaction))\n",
    "            \n",
    "            # sanity check (optional)\n",
    "            if (interaction == 1) && (RBPbase != 0):\n",
    "                locus_id = locus_em.index[0].split('_')[2].split('.fasta')[0]\n",
    "                rbp_id = RBPbase['host_accession']\n",
    "                if locus_id != rbp_id:\n",
    "                    print('mismatch!', rbp_id, locus_id)\n",
    "    \n",
    "    # concatenate to final dataframe\n",
    "    training_dataframe = pd.concat([pd.DataFrame(locus_list), pd.DataFrame(rbp_list), pd.DataFrame(interaction_list)], axis=1)\n",
    "    \n",
    "    return training_dataframe     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "according-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "locus_em = pd.read_csv(data_direct+'/locus_embeddings.csv', index_col=0)\n",
    "frame = construct_training_dataframe(locus_em, rbp_em_mini, int_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "therapeutic-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbpbase = pd.read_csv(data_dir+'/RBPbase_250621_klebsiella_pneumoniae.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-belgium",
   "metadata": {},
   "source": [
    "## X - Out of use"
   ]
  },
  {
   "cell_type": "raw",
   "id": "higher-shelf",
   "metadata": {},
   "source": [
    "def blast_local_python(file_name, data_dir):\n",
    "    \"\"\"\n",
    "    This function does pairwise BLASTs between all sequences of an input FASTA file (Kaptive results)\n",
    "    \n",
    "    Input:\n",
    "    - file_name: string of FASTA file to pairwise BLAST\n",
    "    \n",
    "    To do:\n",
    "    - change to kaptive_file_names -> no big fasta needed, no need to make small fasta's first.\n",
    "    \"\"\"\n",
    "    \n",
    "    sequence_list = []\n",
    "    for record in SeqIO.parse(data_dir+'/'+file_name, 'fasta'):\n",
    "        sequence_list.append(str(record.seq))\n",
    "    score_matrix = np.zeros((len(sequence_list), len(sequence_list)))\n",
    "\n",
    "    for i in range(len(sequence_list)-1):\n",
    "        # make fasta sequence 1\n",
    "        fasta1 = open(data_dir+'/sequence1.fasta', 'w')\n",
    "        fasta1.write('>sequence1'+'\\n'+sequence_list[i]+'\\n')\n",
    "        fasta1.close()\n",
    "        \n",
    "        for j in range(i, len(sequence_list)):\n",
    "            # make fasta sequence 2\n",
    "            fasta2 = open(data_dir+'/sequence2.fasta', 'w')\n",
    "            fasta2.write('>sequence2'+'\\n'+sequence_list[j]+'\\n')\n",
    "            fasta2.close()\n",
    "            \n",
    "            # perform BLAST\n",
    "            blastp_cline = NcbiblastpCommandline(query=data_dir+'/sequence1.fasta', subject=data_dir+'/sequence2.fasta', \n",
    "                                                 evalue=10, outfmt=5, out=data_dir+'/BLAST_results.xml')\n",
    "            stdout, stderr = blastp_cline()\n",
    "            \n",
    "            # get results\n",
    "            blast_result = open(data_dir+'/BLAST_results.xml')\n",
    "            blast_record = NCBIXML.read(blast_result)\n",
    "            try:\n",
    "                alignment = blast_record.alignments[0]\n",
    "                hsp = alignment.hsps[0]\n",
    "                identity = (hsp.identities/hsp.align_length)*100\n",
    "            except IndexError:\n",
    "                identity = 0\n",
    "                pass\n",
    "            \n",
    "            # write to score matrix\n",
    "            score_matrix[i,j] = identity\n",
    "            \n",
    "    score_matrix = score_matrix + score_matrix.transpose()\n",
    "    score_matrix = score_matrix - np.diag(np.diag(score_matrix))  \n",
    "    \n",
    "    os.remove(data_dir+'/sequence1.fasta')\n",
    "    os.remove(data_dir+'/sequence2.fasta')\n",
    "    \n",
    "    return score_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
