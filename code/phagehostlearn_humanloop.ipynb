{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "chronic-establishment",
   "metadata": {},
   "source": [
    "# PhageHostLearn - v3.4.klebsiella - human in the loop\n",
    "\n",
    "An AI-based Phage-Host interaction predictor framework with K-loci and receptor-binding proteins at its core. This particular PhageHostLearn is for *Klebsiella pneumoniae* related phages.\n",
    "\n",
    "This notebook offers the functionality to add new data to the PhageHostLearn framework and retrain the PhageHostLearn prediction models, without having to process all data from scratch. Here, it is assumed that you have completed the initial set-up that is carried out in the `phagehostlearn_training.ipynb`.\n",
    "\n",
    "**Overview of this notebook**\n",
    "- Setting A: new validated interactions for the same data\n",
    "- Setting B or C: new interactions for either new phages or bacteria (= new data)\n",
    "- Setting D: new interactions for new combinations of phages AND bacteria\n",
    "\n",
    "**Architecture of the PhageHostLearn framework**: \n",
    "- Multi-RBP setting: phages consisting of one or more RBPs (multi-instance)\n",
    "- K-loci proteins (multi-instance)\n",
    "- Embeddings for both based on ESM-2 language models and HDC\n",
    "- Combined XGBoost model (for language embeddings) and Random Forest (for HDC embeddings) to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7982b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from os import listdir\n",
    "from joblib import dump, load\n",
    "from tqdm.notebook import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "import phagehostlearn_utils as phlu\n",
    "import phagehostlearn_features as phlf\n",
    "import phagehostlearn_processing as phlp\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf11f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_path = '/Users/dimi/GoogleDrive/PhD/4_PHAGEHOST_LEARNING/42_DATA/Valencia_data'\n",
    "results_path = '/Users/dimi/GoogleDrive/PhD/4_PHAGEHOST_LEARNING/43_RESULTS/models'\n",
    "data_suffix = 'Valencia'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fe187e",
   "metadata": {},
   "source": [
    "## Setting A: adding validated interactions for the same data\n",
    "\n",
    "In this setting, we're not adding new phages or bacterial hosts, but we have tested new interactions for the phages and bacteria that are already present in the dataset. In this scenario, we only need to add those new interactions to our interaction matrix and retrain from there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcfae1a",
   "metadata": {},
   "source": [
    "#### A.1 Manually add the validated interactions in the .xlsx file with interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9893891b",
   "metadata": {},
   "source": [
    "#### A.2 Reconstruct interaction matrix and feature matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e936d0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_xlsx_path = general_path+'/klebsiella_phage_host_interactions.xlsx'\n",
    "phlp.process_interactions(general_path, interactions_xlsx_path, data_suffix=data_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb75f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_esm2, features_hdc, labels, groups_loci, groups_phage = phlf.construct_feature_matrices(general_path, \n",
    "                                                                                            data_suffix=data_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a3c02",
   "metadata": {},
   "source": [
    "#### A.3 Retrain & save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ca0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus=6\n",
    "labels = np.asarray(labels)\n",
    "model_suffix = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf64f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESM-2 FEATURES + XGBoost model\n",
    "imbalance = sum([1 for i in labels if i==1]) / sum([1 for i in labels if i==0])\n",
    "xgb = XGBClassifier(scale_pos_weight=1/imbalance, learning_rate=0.3, n_estimators=250, max_depth=5,\n",
    "                    n_jobs=cpus, eval_metric='logloss', use_label_encoder=False)\n",
    "xgb.fit(features_esm2, labels)\n",
    "xgb.save_model('phagehostlearn_esm2_xgb'+model_suffix+'.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec07c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDC FEATURES + RF model\n",
    "rf = RandomForestClassifier(n_estimators=1000, max_depth=5, class_weight='balanced', n_jobs=cpus)\n",
    "rf.fit(features_hdc, labels)\n",
    "dump(rf, 'phagehostlearn_hdc_rf'+model_suffix+'.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba1d012",
   "metadata": {},
   "source": [
    "## Setting B/C: adding new phages or bacteria + interactions\n",
    "\n",
    "In this setting, we're adding either new phages or bacteria against the known bacteria or phages, respectively. This entails adding the new genomes.fasta files in the respective folders (see `phagehostlearn_training.ipynb`) and manually adding the new rows or columns to the interactions.xlsx file. Alternatively, you can make a new_interactions.xlsx file and combine it with the old interaction matrix in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77015918",
   "metadata": {},
   "source": [
    "#### BC.1 Manually add the new phage genomes or bacterial genomes to their designated folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064c50d5",
   "metadata": {},
   "source": [
    "#### BC.2 Quality check: undetermined nts & identical genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06caa788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genomes with undertermined nts:  ['P4a', 'A1m']\n"
     ]
    }
   ],
   "source": [
    "# undetermined nts\n",
    "phage_genomes_path = general_path+'/phages_genomes'\n",
    "phage_files = listdir(phage_genomes_path)\n",
    "phage_files.remove('.DS_Store')\n",
    "undetermined = []\n",
    "for file in phage_files:\n",
    "    file_dir = phage_genomes_path+'/'+file\n",
    "    for record in SeqIO.parse(file_dir, 'fasta'):\n",
    "        sequence = str(record.seq)\n",
    "        approved_letters = sequence.count('A') + sequence.count('C') + sequence.count('T') + sequence.count('G')\n",
    "        if approved_letters != len(sequence):\n",
    "            undetermined.append(record.id)\n",
    "print('Genomes with undertermined nts: ', undetermined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfbbcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CD-HIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229329aa",
   "metadata": {},
   "source": [
    "#### BC.3 Rerun the relevant processing steps with the add=True parameter\n",
    "\n",
    "If you've added new phage genomes, you'll have to rerun PHANOTATE, constructing protein embeddings and PhageRBPdetect. If you've added new bacterial genomes, you'll have to rerun Kaptive. Afterwards, rerun the processing of the interaction matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb71ba92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  3  more phages (add=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630bcc74a6e44b8099bec95c60835773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PHANOTATE\n",
    "phage_genomes_path = general_path+'/phages_genomes'\n",
    "phanotate_path = '/opt/homebrew/Caskroom/miniforge/base/envs/ML1/bin/phanotate.py'\n",
    "phlp.phanotate_processing(general_path, phage_genomes_path, phanotate_path, data_suffix=data_suffix, add=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62489ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protein embeddings (alternatively run in Google Colab or Kaggle)\n",
    "phlp.compute_protein_embeddings(general_path, data_suffix=data_suffix, add=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a69a6215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Working...    done.\\nPressed and indexed 92 HMMs (92 names and 29 accessions).\\nModels pressed into binary file:   /Users/dimi/GoogleDrive/PhD/4_PHAGEHOST_LEARNING/42_DATA/Valencia_data/RBPdetect_phageRBPs.hmm.h3m\\nSSI index for binary model file:   /Users/dimi/GoogleDrive/PhD/4_PHAGEHOST_LEARNING/42_DATA/Valencia_data/RBPdetect_phageRBPs.hmm.h3i\\nProfiles (MSV part) pressed into:  /Users/dimi/GoogleDrive/PhD/4_PHAGEHOST_LEARNING/42_DATA/Valencia_data/RBPdetect_phageRBPs.hmm.h3f\\nProfiles (remainder) pressed into: /Users/dimi/GoogleDrive/PhD/4_PHAGEHOST_LEARNING/42_DATA/Valencia_data/RBPdetect_phageRBPs.hmm.h3p\\n'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43549fb2d6d24bd2a407db18b4953d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9727 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PhageRBPdetect\n",
    "pfam_path = general_path+'/RBPdetect_phageRBPs.hmm'\n",
    "hmmer_path = '/Users/Dimi/hmmer-3.3.1'\n",
    "xgb_path = general_path+'/RBPdetect_xgb_hmm.json'\n",
    "gene_embeddings_path = general_path+'/phage_protein_embeddings'+data_suffix+'.csv'\n",
    "phlp.phageRBPdetect(general_path, pfam_path, hmmer_path, xgb_path, gene_embeddings_path, data_suffix=data_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaee9554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaptive\n",
    "bact_genomes_path = general_path+'/klebsiella_genomes/fasta_files'\n",
    "kaptive_database_path = general_path+'/Klebsiella_k_locus_primary_reference.gbk'\n",
    "phlp.process_bacterial_genomes(general_path, bact_genomes_path, kaptive_database_path, \n",
    "                               data_suffix=data_suffix, add=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f18c5",
   "metadata": {},
   "source": [
    "#### BC.4 Manually add the new bacteria (as rows) or phages (as columns) and interactions in the interactions.xlsx Excel sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f3c03d",
   "metadata": {},
   "source": [
    "#### BC.5 Reconstruct interaction matrix and feature matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ff86e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_xlsx_path = general_path+'/klebsiella_phage_host_interactions.xlsx'\n",
    "phlp.process_interactions(general_path, interactions_xlsx_path, data_suffix=data_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c447b142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  8  more sequences (add=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [01:34<00:00, 11.86s/it]\n"
     ]
    }
   ],
   "source": [
    "# compute ESM-2 RBP embeddings\n",
    "phlf.compute_esm2_embeddings_rbp(general_path, data_suffix=data_suffix, add=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e6722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute ESM-2 loci embeddings\n",
    "phlf.compute_esm2_embeddings_loci(general_path, data_suffix=data_suffix, add=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03517b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute HDC embeddings\n",
    "locibase_path = general_path+'/Locibase'+data_suffix+'.json'\n",
    "rbpbase_path = general_path+'/RBPbase'+data_suffix+'.csv'\n",
    "phlf.compute_hdc_embedding(general_path, data_suffix, locibase_path, rbpbase_path, mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0855d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp_embeddings_path = general_path+'/esm2_embeddings_rbp'+data_suffix+'.csv'\n",
    "loci_embeddings_path = general_path+'/esm2_embeddings_loci'+data_suffix+'.csv'\n",
    "hdc_embeddings_path = general_path+'/hdc_features'+data_suffix+'.txt'\n",
    "features_esm2, features_hdc, labels, groups_loci, groups_phage = phlf.construct_feature_matrices(general_path, \n",
    "                                                                            data_suffix, loci_embeddings_path, \n",
    "                                                                            rbp_embeddings_path, hdc_embeddings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a25c8b7",
   "metadata": {},
   "source": [
    "#### BC.6 Retrain and save the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330e95d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus = 6\n",
    "labels = np.asarray(labels)\n",
    "model_suffix = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac9e43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESM-2 FEATURES + XGBoost model\n",
    "imbalance = sum([1 for i in labels if i==1]) / sum([1 for i in labels if i==0])\n",
    "xgb = XGBClassifier(scale_pos_weight=1/imbalance, learning_rate=0.3, n_estimators=250, max_depth=5,\n",
    "                    n_jobs=cpus, eval_metric='logloss', use_label_encoder=False)\n",
    "xgb.fit(features_esm2, labels)\n",
    "xgb.save_model('phagehostlearn_esm2_xgb'+model_suffix+'.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c62b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDC FEATURES + RF model\n",
    "rf = RandomForestClassifier(n_estimators=1000, max_depth=5, class_weight='balanced', n_jobs=cpus)\n",
    "rf.fit(features_hdc, labels)\n",
    "dump(rf, 'phagehostlearn_hdc_rf'+model_suffix+'.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3149f2",
   "metadata": {},
   "source": [
    "## Setting D: adding new phages AND bacteria + interactions\n",
    "\n",
    "In this setting, you're adding interactions for new combinations of phages AND bacteria. Here, we will incorporate these new interactions from a new .xlsx file containing the interactions, and then rerun all the processing steps with the add=True argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019e689f",
   "metadata": {},
   "source": [
    "#### D.1 Manually add the new phage genomes and bacterial genomes to their designated folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6462b755",
   "metadata": {},
   "source": [
    "#### D.2 Make a new .xlsx file for the new interactions with the bactrerial accession in the first column (each row a new bacterium) and the phage names in the first row (each column a new phage). Store the .xlsx file in the general folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8388f58b",
   "metadata": {},
   "source": [
    "#### D.3 Rerun all the processing steps with add=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3de244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHANOTATE\n",
    "phage_genomes_path = general_path+'/phages_genomes'\n",
    "phanotate_path = '/opt/homebrew/Caskroom/miniforge/base/envs/ML1/bin/phanotate.py'\n",
    "phlp.phanotate_processing(general_path, phage_genomes_path, phanotate_path, data_suffix=data_suffix, add=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac260445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protein embeddings (alternatively run in Google Colab or Kaggle)\n",
    "phlp.compute_protein_embeddings(general_path, data_suffix=data_suffix, add=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447b24e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PhageRBPdetect\n",
    "pfam_path = general_path+'/RBPdetect_phageRBPs.hmm'\n",
    "hmmer_path = '/Users/Dimi/hmmer-3.3.1'\n",
    "xgb_path = general_path+'/RBPdetect_xgb_hmm.json'\n",
    "gene_embeddings_path = general_path+'/phage_protein_embeddings'+data_suffix+'.csv'\n",
    "phlp.phageRBPdetect(general_path, pfam_path, hmmer_path, xgb_path, gene_embeddings_path, data_suffix=data_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d597c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaptive\n",
    "bact_genomes_path = general_path+'/klebsiella_genomes/fasta_files'\n",
    "kaptive_database_path = general_path+'/Klebsiella_k_locus_primary_reference.gbk'\n",
    "phlp.process_bacterial_genomes(general_path, bact_genomes_path, kaptive_database_path, \n",
    "                               data_suffix=data_suffix, add=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8214147f",
   "metadata": {},
   "source": [
    "#### D.4 Integrate the interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a581aca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_interactions_xlsx_path = general_path+'/klebsiella_phage_host_interactions.xlsx'\n",
    "new_interactions_xlsx_path = ...\n",
    "interactions_path = general_path+'/phage_host_interactions'+data_suffix\n",
    "phlp.process_interactions(general_path, original_interactions_xlsx_path, data_suffix=data_suffix)\n",
    "phlp.add_to_database(interactions_path+'.csv', new_interactions_xlsx_path, interactions_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6c76b9",
   "metadata": {},
   "source": [
    "#### D.5 Reconstruct features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ec8b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute ESM-2 RBP embeddings\n",
    "phlf.compute_esm2_embeddings_rbp(general_path, data_suffix=data_suffix, add=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d2469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute ESM-2 loci embeddings\n",
    "phlf.compute_esm2_embeddings_loci(general_path, data_suffix=data_suffix, add=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8073c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute HDC embeddings\n",
    "locibase_path = general_path+'/Locibase'+data_suffix+'.json'\n",
    "rbpbase_path = general_path+'/RBPbase'+data_suffix+'.csv'\n",
    "phlf.compute_hdc_embedding(general_path, data_suffix, locibase_path, rbpbase_path, mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b24e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_esm2, features_hdc, labels, groups_loci, groups_phage = phlf.construct_feature_matrices(general_path, \n",
    "                                                                                            data_suffix=data_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697a93a4",
   "metadata": {},
   "source": [
    "#### D.6 Retrain and save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c655cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus = 6\n",
    "labels = np.asarray(labels)\n",
    "model_suffix = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead91f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESM-2 FEATURES + XGBoost model\n",
    "imbalance = sum([1 for i in labels if i==1]) / sum([1 for i in labels if i==0])\n",
    "xgb = XGBClassifier(scale_pos_weight=1/imbalance, learning_rate=0.3, n_estimators=250, max_depth=5,\n",
    "                    n_jobs=cpus, eval_metric='logloss', use_label_encoder=False)\n",
    "xgb.fit(features_esm2, labels)\n",
    "xgb.save_model('phagehostlearn_esm2_xgb'+model_suffix+'.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52041e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDC FEATURES + RF model\n",
    "rf = RandomForestClassifier(n_estimators=1000, max_depth=5, class_weight='balanced', n_jobs=cpus)\n",
    "rf.fit(features_hdc, labels)\n",
    "dump(rf, 'phagehostlearn_hdc_rf'+model_suffix+'.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dec998",
   "metadata": {},
   "source": [
    "## Evaluation: LOGOCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02356708",
   "metadata": {},
   "outputs": [],
   "source": [
    "logo = LeaveOneGroupOut()\n",
    "cpus = 6\n",
    "scores_lan = []\n",
    "scores_hdc = []\n",
    "label_list = []\n",
    "labels = np.asarray(labels)\n",
    "pbar = tqdm(total=len(set(groups_loci)))\n",
    "for train_index, test_index in logo.split(features_esm2, labels, groups_loci):\n",
    "    # get the training and test data\n",
    "    Xlan_train, Xlan_test = features_esm2[train_index], features_esm2[test_index]\n",
    "    Xhdc_train, Xhdc_test = features_hdc[train_index], features_hdc[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    imbalance = sum([1 for i in y_train if i==1]) / sum([1 for i in y_train if i==0])\n",
    "\n",
    "    ## ESM-2 EMBEDDINGS: XGBoost model\n",
    "    xgb = XGBClassifier(scale_pos_weight=1/imbalance, learning_rate=0.3, n_estimators=250, max_depth=7,\n",
    "                        n_jobs=cpus, eval_metric='logloss', use_label_encoder=False)\n",
    "    xgb.fit(Xlan_train, y_train)\n",
    "    score_xgb = xgb.predict_proba(Xlan_test)[:,1]\n",
    "    scores_lan.append(score_xgb)\n",
    "    \n",
    "    ## HDC EMBEDDINGS: RF model\n",
    "    rf = RandomForestClassifier(n_estimators=1250, max_depth=3, class_weight='balanced', n_jobs=cpus)\n",
    "    rf.fit(Xhdc_train, y_train)\n",
    "    score_rf = rf.predict_proba(Xhdc_test)[:,1]\n",
    "    scores_hdc.append(score_rf)\n",
    "    \n",
    "    # save labels for later\n",
    "    label_list.append(y_test)\n",
    "    \n",
    "    # pbar update\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fdf6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "logo_results = {'labels': label_list, 'scores_language': scores_lan, 'scores_hdc': scores_hdc}   \n",
    "with open(results_path+'/v3.4/combined_logocv_results_v34celia.pickle', 'wb') as f:\n",
    "    pickle.dump(logo_results, f)\n",
    "    \n",
    "# read results\n",
    "#with open(results_path+'/v3.3/combined_logocv_results.pickle', 'rb') as f:\n",
    "#    logo_results = pickle.load(f)\n",
    "#scores_lan = logo_results['scores_language']\n",
    "#scores_hdc = logo_results['scores_hdc']\n",
    "#labels = logo_results['labels']\n",
    "\n",
    "# compute performance\n",
    "rocaucs = []\n",
    "praucs = []\n",
    "rqueries_lan = []\n",
    "rqueries_hdc = []\n",
    "rqueries_ens = []\n",
    "for i in range(len(set(groups_loci))):\n",
    "    score_lan = scores_lan[i]\n",
    "    score_hdc = scores_hdc[i]\n",
    "    score_ens = [phlu.uninorm(score_lan[j], score_hdc[j]) for j in range(len(score_lan))]\n",
    "    y_test = label_list[i]\n",
    "    try:\n",
    "            roc_auc = roc_auc_score(y_test, score_ens)\n",
    "            rocaucs.append(roc_auc)\n",
    "            precision, recall, thresholds = precision_recall_curve(y_test, score_ens)\n",
    "            praucs.append(round(auc(recall, precision), 3))\n",
    "            ranked_lan = [x for _, x in sorted(zip(score_lan, y_test), reverse=True)]\n",
    "            ranked_hdc = [x for _, x in sorted(zip(score_hdc, y_test), reverse=True)]\n",
    "            ranked_ens = [x for _, x in sorted(zip(score_ens, y_test), reverse=True)]\n",
    "            rqueries_lan.append(ranked_lan)\n",
    "            rqueries_hdc.append(ranked_hdc)\n",
    "            rqueries_ens.append(ranked_ens)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63bd936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results, part 1\n",
    "print('Mean average recall @ K:')\n",
    "print('ESM-2 + XGBoost: ', phlu.marecallatk(rqueries_lan, 50))\n",
    "print('HDC + Random Forest: ', phlu.marecallatk(rqueries_hdc, 50))\n",
    "print('Combined model: ', phlu.marecallatk(rqueries_ens, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70411a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results, hit ratios @ K\n",
    "ks = np.linspace(1, 50, 50)\n",
    "hits_lan = [phlu.hitratio(rqueries_lan, int(k)) for k in ks]\n",
    "hits_hdc = [phlu.hitratio(rqueries_hdc, int(k)) for k in ks]\n",
    "hits_ens = [phlu.hitratio(rqueries_ens, int(k)) for k in ks]\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "ax.plot(ks, hits_lan, c='#E15554', linewidth=2.5, ls=':', label='ESM-2 + XGBoost')\n",
    "ax.plot(ks, hits_hdc, c='#E15554', linewidth=2.5, ls='-.', label='HDC + Random Forest')\n",
    "ax.plot(ks, hits_ens, c='#124559', linewidth=2.5, label='Combined model')\n",
    "ax.set_xlabel('$\\it{K}$', size=12)\n",
    "ax.set_ylabel('Hit ratio @ $\\it{K}$', size=12)\n",
    "ax.set_ylim(0.1, 1)\n",
    "ax.legend(loc=4, prop={'size': 12})\n",
    "ax.grid(True)\n",
    "fig.savefig(results_path+'/v3.4/logocv_hitratio_v34celia.png', dpi=400)\n",
    "fig.savefig(results_path+'/v3.4/logocv_hitratio_svg_v34celia.svg', format='svg', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72835ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results, recalls @ K\n",
    "ks = np.linspace(1, 50, 50)\n",
    "recalls_lan = [phlu.recallatk(rqueries_lan, int(k)) for k in ks]\n",
    "recalls_hdc = [phlu.recallatk(rqueries_hdc, int(k)) for k in ks]\n",
    "recalls_ens = [phlu.recallatk(rqueries_ens, int(k)) for k in ks]\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "ax.plot(ks, recalls_lan, c='#E15554', linewidth=2.5, ls=':', label='ESM-2 + XGBoost')\n",
    "ax.plot(ks, recalls_hdc, c='#E15554', linewidth=2.5, ls='-.', label='HDC + Random Forest')\n",
    "ax.plot(ks, recalls_ens, c='#124559', linewidth=2.5, label='Combined model')\n",
    "ax.set_xlabel('$\\it{K}$', size=12)\n",
    "ax.set_ylabel('Recall @ $\\it{K}$', size=12)\n",
    "ax.set_ylim(0.1, 1)\n",
    "ax.legend(loc=4, prop={'size': 12})\n",
    "ax.grid(True)\n",
    "fig.savefig(results_path+'/v3.4/logocv_recall_v34celia.png', dpi=400)\n",
    "fig.savefig(results_path+'/v3.4/logocv_recall_svg_v34celia.svg', format='svg', dpi=400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "cdf400e2b2cf645b2ec6a448fcb5b3c1b3d3d5834714944466c9c0370880fa51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
