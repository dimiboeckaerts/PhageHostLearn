{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f577d7df",
   "metadata": {},
   "source": [
    "# PhageHostLearn - v2.2 (Klebsiella)\n",
    "\n",
    "An AI-based Phage-Host interaction predictor framework with K-loci and receptor-binding proteins at its core. This particular PhageHostLearn is for *Klebsiella pneumoniae* related phages. This notebook follows after having ran the PhageHostLearn_processing steps implemented in the accompanying Jupyter notebook.\n",
    "\n",
    "**Architecture of this framework**: \n",
    "- Multi-RBP setting: phages consisting of one or more RBPs (multi-instance)\n",
    "- K-loci proteins (multi-instance) \n",
    "- Hyoerdimensional vector embeddings for both that are bound or concatenated\n",
    "- Simple cosine similarity \n",
    "- Train an XGBoost and RF model on the concatenated embeddings (binary classification)\n",
    "\n",
    "**Overview of the notebook**:\n",
    "\n",
    "1. [Defining the necessary functions](#functions)\n",
    "2. [Transform the loci sequences into embeddings](#lociembed) using hyperdimensional vectors\n",
    "3. [Transform the RBP sequences into embeddings](#rbpembed) using hyperdimensional vectors\n",
    "4. [Compute joint features: bind or concat](#joint)\n",
    "5. [PCA and RF](#machinelearning)\n",
    "5. [Exploration of the embeddings](#explore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cc1380",
   "metadata": {},
   "source": [
    "## 0 - Libraries and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5e3e26c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant PCA. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant RandomForestClassifier. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant roc_auc_score. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    }
   ],
   "source": [
    "using CSV\n",
    "using MLJ\n",
    "using Test\n",
    "using JSON\n",
    "using Plots\n",
    "using FASTX\n",
    "using MLBase\n",
    "using Colors\n",
    "using Random\n",
    "using XGBoost\n",
    "using StatsBase\n",
    "using Statistics\n",
    "using DataFrames\n",
    "using ScikitLearn\n",
    "using BioAlignments\n",
    "using ProgressMeter\n",
    "using LinearAlgebra\n",
    "using ProgressMeter\n",
    "using BioAlignments\n",
    "using DelimitedFiles\n",
    "using MultivariateStats\n",
    "using ScikitLearn.Pipelines: Pipeline\n",
    "using ScikitLearn.CrossValidation: train_test_split \n",
    "using ScikitLearn.GridSearch: GridSearchCV\n",
    "using ScikitLearn.Skcore: make_scorer\n",
    "\n",
    "@sk_import decomposition: PCA\n",
    "@sk_import ensemble: RandomForestClassifier\n",
    "@sk_import metrics: roc_auc_score\n",
    "\n",
    "push!(LOAD_PATH, \"/Users/dimi/Documents/GitHub/HyperdimensionalComputing.jl/src/\")\n",
    "using HyperdimensionalComputing\n",
    "\n",
    "general_dir = \"/Users/dimi/GoogleDrive/PhD/4_PHAGEHOST_LEARNING/42_DATA/Valencia_data\" # general directory\n",
    "results_dir = \"/Users/dimi/GoogleDrive/PhD/4_PHAGEHOST_LEARNING/43_RESULTS/models\"\n",
    "data_suffix = \"Valencia\"; # choose a suffix for the created data files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cff52b",
   "metadata": {},
   "source": [
    "## 1 - Functions<a name=\"functions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "015ad564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file_to_array (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function file_to_array(file)\n",
    "    \"\"\"\n",
    "    Function that reads a FASTA file and puts its sequences in an array.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    reader = FASTA.Reader(open(file, \"r\"))\n",
    "    for record in reader\n",
    "        seq = FASTA.sequence(record)\n",
    "        push!(sequences, seq)\n",
    "    end\n",
    "    return sequences\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc871105",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean_reciprocal_rank (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mean_reciprocal_rank(queries)\n",
    "    \"\"\"\n",
    "    This function computes the mean reciprocal rank for a given array or\n",
    "    matrix of queries. It deals with relevant vs. non-relevant queries that are\n",
    "    binary. If queries is a matrix, then it will compute the reciprocal ranks over\n",
    "    all rows individually (for each 'query') and then average those.\n",
    "    E.g.:\n",
    "    queries = [[0, 0, 0], [0, 1, 0], [1, 0, 0]]\n",
    "    mean_reciprocal_rank(queries) -> 0.5\n",
    "    \"\"\"\n",
    "    if isa(queries, Matrix)\n",
    "        queries_list = [queries[i,:] for i in 1:size(queries)[1]]\n",
    "        reciprocal_ranks = [sum(query) > 0 ? 1/argmax(query) : 0 for query in queries_list]\n",
    "    else\n",
    "        reciprocal_ranks = [sum(query) > 0 ? 1/argmax(query) : 0 for query in queries]\n",
    "    end\n",
    "    return mean(reciprocal_ranks)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f3be05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sort_label_matrix (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sort_label_matrix(score_matrix, label_matrix)\n",
    "    \"\"\"\n",
    "    This function sorts the label matrix based on the score matrix.\n",
    "    It does so per row (corresponding to bacteria in our setting).\n",
    "    \n",
    "    WARNING: The sortperm function will rank equal elements by ascending index order. \n",
    "    This potentially can cause an underestimation of performance (MRR), as it can be that \n",
    "    an irrelevant 0 appears earlier and then is place before a relevant 1.\n",
    "    \"\"\"\n",
    "    @assert size(label_matrix) == size(score_matrix)\n",
    "    sorted_matrix = zeros(Int64, size(label_matrix)[1], size(label_matrix)[2])\n",
    "    for i in 1:size(label_matrix)[1] # loop over rows\n",
    "        label_row = label_matrix[i,:]\n",
    "        score_row = score_matrix[i,:]\n",
    "        sorted_row = label_row[sortperm(score_row, rev=true)]\n",
    "        sorted_matrix[i,:] = sorted_row\n",
    "    end\n",
    "    return sorted_matrix\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "596df09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mrr_from_scores (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mrr_from_scores(score_matrix, label_matrix)\n",
    "    \"\"\"\n",
    "    This function is a wrapper for the previous functions that computes\n",
    "    the MRR starting from a matrix of (prediction) scores, a threshold above\n",
    "    which to consider an interaction positive and a label matrix.\n",
    "    \"\"\"\n",
    "    # sort the matrix per row and compute\n",
    "    replace!(label_matrix, missing => 0)\n",
    "    sorted_matrix = sort_label_matrix(score_matrix, label_matrix)\n",
    "    \n",
    "    return mean_reciprocal_rank(sorted_matrix)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "916c4a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "auc_from_scores (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function auc_from_scores(scores_flat, labels_flat)\n",
    "    \"\"\"\n",
    "    This function computes the AUC from raw scores returned by ScikitLearn classifiers.\n",
    "    For the AUC, we need the flat versions of scores and labels that don't contain missing \n",
    "    values (if not, this will skew the results).\n",
    "    \n",
    "    Dependencies: MLJ\n",
    "    \"\"\"\n",
    "    c = [\"neg\", \"pos\"]\n",
    "    labels_cat = categorical(c[labels_flat .+ 1])\n",
    "    scores_uni = [UnivariateFinite(categorical([\"neg\", \"pos\"]), [1.0 - p, p]) for p in scores_flat]\n",
    "    AUC = auc(scores_uni, labels_cat)\n",
    "    return AUC\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7123b0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_performance (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function compute_performance(score_matrix, label_matrix, scores_flat, labels_flat)\n",
    "    \"\"\"\n",
    "    Wrapper function that computes the two relevant results we want to compare for \n",
    "    our models: the ROC AUC and the MRR.    \n",
    "    \"\"\"\n",
    "    MRR = mrr_from_scores(score_matrix, label_matrix) # compute MRR\n",
    "    AUC = auc_from_scores(scores_flat, labels_flat) # compute AUC\n",
    "    return MRR, AUC\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1db0834",
   "metadata": {},
   "source": [
    "## 2 - Computing loci embeddings<a name=\"lociembed\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4dcb22",
   "metadata": {},
   "source": [
    "In this section, we define hyperdimensional vectors for the amino-acid alphabet and use these hyperdimensional vectors to construct *hyperdimensional embeddings* for our loci proteins. For the loci proteins, this is a multi-instance setting: multiple proteins will be embedded into hyperdimensional space and then those vectors are aggregated to form one final vector for each locus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69da2bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "LociBase = JSON.parsefile(general_dir*\"/Locibase\"*data_suffix*\".json\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb78964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define protein alphabet\n",
    "alphabet = \"GAVLIFPSTYCMKRHWDENQX\"\n",
    "basis = Dict(c=>BipolarHDV() for c in alphabet);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "356e2c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute loci embeddings w/ proteins (multi-instance)\n",
    "loci_embeddings = Array{BipolarHDV}(undef, length(LociBase))\n",
    "for (i, (name, proteins)) in enumerate(LociBase)\n",
    "    # bind within one sequence, then aggregate the different sequences\n",
    "    protein_hdvs = [sequence_embedding(string(sequence), basis, 6) for sequence in proteins]\n",
    "    loci_hdv = HyperdimensionalComputing.aggregate(protein_hdvs)\n",
    "    loci_embeddings[i] = loci_hdv\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a573ebc0",
   "metadata": {},
   "source": [
    "## 3 - Computing RBP embeddings<a name=\"rbpembed\"></a>\n",
    "\n",
    "We combine the vectors for each phage's RBP(s), also a multi-instance setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b646f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "RBPbase = DataFrame(CSV.File(general_dir*\"/RBPbase\"*data_suffix*\".csv\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "982319c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute rbp embeddings\n",
    "rbp_embeddings = Array{BipolarHDV}(undef, length(unique(RBPbase.phage_ID)))\n",
    "for (i, phageid) in enumerate(unique(RBPbase.phage_ID))\n",
    "    subset = filter(row -> row.phage_ID == phageid, RBPbase)\n",
    "    protein_hdvs = [sequence_embedding(string(sequence), basis, 6) for sequence in subset.protein_sequence]\n",
    "    multirbp_hdv = HyperdimensionalComputing.aggregate(protein_hdvs)\n",
    "    rbp_embeddings[i] = multirbp_hdv\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e46957e",
   "metadata": {},
   "source": [
    "## 4 - Compute joint feature representations and make dataframe for training<a name=\"joint\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f71dbd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "IM = DataFrame(CSV.File(general_dir*\"/phage_host_interactions\"*data_suffix*\".csv\"))\n",
    "rename!(IM,:Column1 => :Host)\n",
    "interaction_matrix = Matrix(IM[1:end, 2:end])\n",
    "#loci_names = IM.accession\n",
    "#serotypes = DataFrame(CSV.File(general_dir*\"/serotypes\"*data_suffix*\".csv\"))\n",
    "#rbp_names = names(IM)[2:end];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58156925",
   "metadata": {},
   "source": [
    "##### Binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4cb21946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute sigatures for loci x RBP embeddings\n",
    "features_bind = []\n",
    "labels = []\n",
    "for (i, accession) in enumerate(collect(keys(LociBase)))\n",
    "    for (j, phage_id) in enumerate(unique(RBPbase.phage_ID))\n",
    "        subset = filter(row -> row.Host == accession, IM)\n",
    "        interaction = subset[!, phage_id][1]\n",
    "        if isequal(interaction, 1) || isequal(interaction, 0)\n",
    "            signature = HyperdimensionalComputing.bind([loci_embeddings[i], rbp_embeddings[j]])\n",
    "            push!(features_bind, signature)\n",
    "            push!(labels, trunc(Int, interaction))\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "01c70737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the signatures in a matrix for sklearn\n",
    "features_b = zeros(Int64, length(features_bind), 10000)\n",
    "for i in range(1, length=length(features_bind))\n",
    "    features_b[i,:] = features_bind[i]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7377488",
   "metadata": {},
   "source": [
    "#### Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c25d9706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute [loci] + [RBP] embeddings\n",
    "features_concat = []\n",
    "labels = []\n",
    "for (i, accession) in enumerate(collect(keys(LociBase)))\n",
    "    for (j, phage_id) in enumerate(unique(RBPbase.phage_ID))\n",
    "        subset = filter(row -> row.Host == accession, IM)\n",
    "        interaction = subset[!, phage_id][1]\n",
    "        if isequal(interaction, 1) || isequal(interaction, 0)\n",
    "            concatenation = vcat(loci_embeddings[i], rbp_embeddings[j])\n",
    "            push!(features_concat, concatenation)\n",
    "            push!(labels, trunc(Int, interaction))\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "633832b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the signatures in a matrix for sklearn\n",
    "features_c = zeros(Int64, length(features_concat), 20000)\n",
    "for i in range(1, length=length(features_concat))\n",
    "    features_c[i,:] = features_concat[i]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d4aa51",
   "metadata": {},
   "source": [
    "## 5 - Do PCA and build RF model<a name=\"machinelearning\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abe932b",
   "metadata": {},
   "source": [
    "##### Binding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2e38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle and set test set aside\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_b, labels, test_size=0.2, stratify=labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44377ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model and CV\n",
    "rf = RandomForestClassifier(class_weight=\"balanced\")\n",
    "pca = PCA()\n",
    "pipe = Pipeline([(\"pca\", pca), (\"rf\", rf)])\n",
    "params_pipe = Dict(\"pca__n_components\"=>[50, 100, 200, 300], \"rf__n_estimators\"=>[100, 250, 500, 750])\n",
    "cv = CrossValidation.StratifiedKFold(y_train, n_folds=5, shuffle=true, random_state=42);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "00c1591a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] rf__n_estimators=100, pca__n_components=50\n",
      "[CV] rf__n_estimators=100, pca__n_components=50, score=0.63990  -  5.0s\n",
      "[CV] rf__n_estimators=100, pca__n_components=50\n",
      "[CV] rf__n_estimators=100, pca__n_components=50, score=0.66162  -  4.6s\n",
      "[CV] rf__n_estimators=100, pca__n_components=50\n",
      "[CV] rf__n_estimators=100, pca__n_components=50, score=0.61982  -  4.8s\n",
      "[CV] rf__n_estimators=100, pca__n_components=50\n",
      "[CV] rf__n_estimators=100, pca__n_components=50, score=0.67314  -  4.9s\n",
      "[CV] rf__n_estimators=100, pca__n_components=50\n",
      "[CV] rf__n_estimators=100, pca__n_components=50, score=0.62106  -  4.8s\n",
      "[CV] rf__n_estimators=250, pca__n_components=50\n",
      "[CV] rf__n_estimators=250, pca__n_components=50, score=0.64023  -  7.3s\n",
      "[CV] rf__n_estimators=250, pca__n_components=50\n",
      "[CV] rf__n_estimators=250, pca__n_components=50, score=0.66130  -  7.2s\n",
      "[CV] rf__n_estimators=250, pca__n_components=50\n",
      "[CV] rf__n_estimators=250, pca__n_components=50, score=0.61949  -  7.4s\n",
      "[CV] rf__n_estimators=250, pca__n_components=50\n",
      "[CV] rf__n_estimators=250, pca__n_components=50, score=0.67314  -  7.2s\n",
      "[CV] rf__n_estimators=250, pca__n_components=50\n",
      "[CV] rf__n_estimators=250, pca__n_components=50, score=0.62106  -  7.6s\n",
      "[CV] rf__n_estimators=500, pca__n_components=50\n",
      "[CV] rf__n_estimators=500, pca__n_components=50, score=0.64023  -  11.6s\n",
      "[CV] rf__n_estimators=500, pca__n_components=50\n",
      "[CV] rf__n_estimators=500, pca__n_components=50, score=0.66162  -  11.7s\n",
      "[CV] rf__n_estimators=500, pca__n_components=50\n",
      "[CV] rf__n_estimators=500, pca__n_components=50, score=0.61949  -  11.4s\n",
      "[CV] rf__n_estimators=500, pca__n_components=50\n",
      "[CV] rf__n_estimators=500, pca__n_components=50, score=0.67314  -  11.7s\n",
      "[CV] rf__n_estimators=500, pca__n_components=50\n",
      "[CV] rf__n_estimators=500, pca__n_components=50, score=0.62106  -  11.9s\n",
      "[CV] rf__n_estimators=750, pca__n_components=50\n",
      "[CV] rf__n_estimators=750, pca__n_components=50, score=0.64023  -  16.1s\n",
      "[CV] rf__n_estimators=750, pca__n_components=50\n",
      "[CV] rf__n_estimators=750, pca__n_components=50, score=0.66162  -  16.0s\n",
      "[CV] rf__n_estimators=750, pca__n_components=50\n",
      "[CV] rf__n_estimators=750, pca__n_components=50, score=0.62970  -  15.8s\n",
      "[CV] rf__n_estimators=750, pca__n_components=50\n",
      "[CV] rf__n_estimators=750, pca__n_components=50, score=0.67281  -  15.8s\n",
      "[CV] rf__n_estimators=750, pca__n_components=50\n",
      "[CV] rf__n_estimators=750, pca__n_components=50, score=0.62106  -  16.2s\n",
      "[CV] rf__n_estimators=100, pca__n_components=100\n",
      "[CV] rf__n_estimators=100, pca__n_components=100, score=0.65043  -  7.1s\n",
      "[CV] rf__n_estimators=100, pca__n_components=100\n",
      "[CV] rf__n_estimators=100, pca__n_components=100, score=0.66162  -  7.4s\n",
      "[CV] rf__n_estimators=100, pca__n_components=100\n",
      "[CV] rf__n_estimators=100, pca__n_components=100, score=0.61982  -  7.3s\n",
      "[CV] rf__n_estimators=100, pca__n_components=100\n",
      "[CV] rf__n_estimators=100, pca__n_components=100, score=0.68323  -  7.5s\n",
      "[CV] rf__n_estimators=100, pca__n_components=100\n",
      "[CV] rf__n_estimators=100, pca__n_components=100, score=0.64189  -  7.5s\n",
      "[CV] rf__n_estimators=250, pca__n_components=100\n",
      "[CV] rf__n_estimators=250, pca__n_components=100, score=0.64023  -  11.5s\n",
      "[CV] rf__n_estimators=250, pca__n_components=100\n",
      "[CV] rf__n_estimators=250, pca__n_components=100, score=0.66130  -  11.1s\n",
      "[CV] rf__n_estimators=250, pca__n_components=100\n",
      "[CV] rf__n_estimators=250, pca__n_components=100, score=0.62970  -  11.0s\n",
      "[CV] rf__n_estimators=250, pca__n_components=100\n",
      "[CV] rf__n_estimators=250, pca__n_components=100, score=0.67281  -  11.1s\n",
      "[CV] rf__n_estimators=250, pca__n_components=100\n",
      "[CV] rf__n_estimators=250, pca__n_components=100, score=0.63147  -  11.2s\n",
      "[CV] rf__n_estimators=500, pca__n_components=100\n",
      "[CV] rf__n_estimators=500, pca__n_components=100, score=0.64023  -  17.4s\n",
      "[CV] rf__n_estimators=500, pca__n_components=100\n",
      "[CV] rf__n_estimators=500, pca__n_components=100, score=0.66162  -  17.2s\n",
      "[CV] rf__n_estimators=500, pca__n_components=100\n",
      "[CV] rf__n_estimators=500, pca__n_components=100, score=0.62970  -  17.0s\n",
      "[CV] rf__n_estimators=500, pca__n_components=100\n",
      "[CV] rf__n_estimators=500, pca__n_components=100, score=0.68356  -  17.1s\n",
      "[CV] rf__n_estimators=500, pca__n_components=100\n",
      "[CV] rf__n_estimators=500, pca__n_components=100, score=0.63147  -  17.1s\n",
      "[CV] rf__n_estimators=750, pca__n_components=100\n",
      "[CV] rf__n_estimators=750, pca__n_components=100, score=0.64023  -  23.7s\n",
      "[CV] rf__n_estimators=750, pca__n_components=100\n",
      "[CV] rf__n_estimators=750, pca__n_components=100, score=0.66162  -  23.6s\n",
      "[CV] rf__n_estimators=750, pca__n_components=100\n",
      "[CV] rf__n_estimators=750, pca__n_components=100, score=0.62970  -  23.6s\n",
      "[CV] rf__n_estimators=750, pca__n_components=100\n",
      "[CV] rf__n_estimators=750, pca__n_components=100, score=0.68356  -  23.3s\n",
      "[CV] rf__n_estimators=750, pca__n_components=100\n",
      "[CV] rf__n_estimators=750, pca__n_components=100, score=0.64189  -  23.4s\n",
      "[CV] rf__n_estimators=100, pca__n_components=200\n",
      "[CV] rf__n_estimators=100, pca__n_components=200, score=0.64023  -  11.8s\n",
      "[CV] rf__n_estimators=100, pca__n_components=200\n",
      "[CV] rf__n_estimators=100, pca__n_components=200, score=0.66162  -  11.9s\n",
      "[CV] rf__n_estimators=100, pca__n_components=200\n",
      "[CV] rf__n_estimators=100, pca__n_components=200, score=0.62970  -  12.3s\n",
      "[CV] rf__n_estimators=100, pca__n_components=200\n",
      "[CV] rf__n_estimators=100, pca__n_components=200, score=0.71481  -  12.2s\n",
      "[CV] rf__n_estimators=100, pca__n_components=200\n",
      "[CV] rf__n_estimators=100, pca__n_components=200, score=0.64189  -  12.2s\n",
      "[CV] rf__n_estimators=250, pca__n_components=200\n",
      "[CV] rf__n_estimators=250, pca__n_components=200, score=0.64023  -  17.9s\n",
      "[CV] rf__n_estimators=250, pca__n_components=200\n",
      "[CV] rf__n_estimators=250, pca__n_components=200, score=0.66162  -  17.5s\n",
      "[CV] rf__n_estimators=250, pca__n_components=200\n",
      "[CV] rf__n_estimators=250, pca__n_components=200, score=0.62970  -  17.8s\n",
      "[CV] rf__n_estimators=250, pca__n_components=200\n",
      "[CV] rf__n_estimators=250, pca__n_components=200, score=0.71481  -  16.9s\n",
      "[CV] rf__n_estimators=250, pca__n_components=200\n",
      "[CV] rf__n_estimators=250, pca__n_components=200, score=0.64189  -  17.8s\n",
      "[CV] rf__n_estimators=500, pca__n_components=200\n",
      "[CV] rf__n_estimators=500, pca__n_components=200, score=0.64023  -  26.6s\n",
      "[CV] rf__n_estimators=500, pca__n_components=200\n",
      "[CV] rf__n_estimators=500, pca__n_components=200, score=0.66162  -  25.2s\n",
      "[CV] rf__n_estimators=500, pca__n_components=200\n",
      "[CV] rf__n_estimators=500, pca__n_components=200, score=0.62970  -  26.1s\n",
      "[CV] rf__n_estimators=500, pca__n_components=200\n",
      "[CV] rf__n_estimators=500, pca__n_components=200, score=0.71481  -  28.5s\n",
      "[CV] rf__n_estimators=500, pca__n_components=200\n",
      "[CV] rf__n_estimators=500, pca__n_components=200, score=0.64189  -  28.1s\n",
      "[CV] rf__n_estimators=750, pca__n_components=200\n",
      "[CV] rf__n_estimators=750, pca__n_components=200, score=0.64023  -  34.9s\n",
      "[CV] rf__n_estimators=750, pca__n_components=200\n",
      "[CV] rf__n_estimators=750, pca__n_components=200, score=0.66162  -  33.9s\n",
      "[CV] rf__n_estimators=750, pca__n_components=200\n",
      "[CV] rf__n_estimators=750, pca__n_components=200, score=0.62970  -  34.1s\n",
      "[CV] rf__n_estimators=750, pca__n_components=200\n",
      "[CV] rf__n_estimators=750, pca__n_components=200, score=0.70439  -  33.6s\n",
      "[CV] rf__n_estimators=750, pca__n_components=200\n",
      "[CV] rf__n_estimators=750, pca__n_components=200, score=0.63147  -  34.0s\n",
      "[CV] rf__n_estimators=100, pca__n_components=300\n",
      "[CV] rf__n_estimators=100, pca__n_components=300, score=0.64023  -  15.8s\n",
      "[CV] rf__n_estimators=100, pca__n_components=300\n",
      "[CV] rf__n_estimators=100, pca__n_components=300, score=0.66162  -  16.4s\n",
      "[CV] rf__n_estimators=100, pca__n_components=300\n",
      "[CV] rf__n_estimators=100, pca__n_components=300, score=0.62970  -  16.4s\n",
      "[CV] rf__n_estimators=100, pca__n_components=300\n",
      "[CV] rf__n_estimators=100, pca__n_components=300, score=0.68356  -  17.1s\n",
      "[CV] rf__n_estimators=100, pca__n_components=300\n",
      "[CV] rf__n_estimators=100, pca__n_components=300, score=0.63147  -  17.0s\n",
      "[CV] rf__n_estimators=250, pca__n_components=300\n",
      "[CV] rf__n_estimators=250, pca__n_components=300, score=0.64023  -  24.3s\n",
      "[CV] rf__n_estimators=250, pca__n_components=300\n",
      "[CV] rf__n_estimators=250, pca__n_components=300, score=0.66162  -  23.6s\n",
      "[CV] rf__n_estimators=250, pca__n_components=300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] rf__n_estimators=250, pca__n_components=300, score=0.62970  -  23.5s\n",
      "[CV] rf__n_estimators=250, pca__n_components=300\n",
      "[CV] rf__n_estimators=250, pca__n_components=300, score=0.69397  -  23.5s\n",
      "[CV] rf__n_estimators=250, pca__n_components=300\n",
      "[CV] rf__n_estimators=250, pca__n_components=300, score=0.63147  -  24.5s\n",
      "[CV] rf__n_estimators=500, pca__n_components=300\n",
      "[CV] rf__n_estimators=500, pca__n_components=300, score=0.64023  -  34.7s\n",
      "[CV] rf__n_estimators=500, pca__n_components=300\n",
      "[CV] rf__n_estimators=500, pca__n_components=300, score=0.66162  -  34.2s\n",
      "[CV] rf__n_estimators=500, pca__n_components=300\n",
      "[CV] rf__n_estimators=500, pca__n_components=300, score=0.62970  -  32.8s\n",
      "[CV] rf__n_estimators=500, pca__n_components=300\n",
      "[CV] rf__n_estimators=500, pca__n_components=300, score=0.70439  -  33.5s\n",
      "[CV] rf__n_estimators=500, pca__n_components=300\n",
      "[CV] rf__n_estimators=500, pca__n_components=300, score=0.64189  -  34.0s\n",
      "[CV] rf__n_estimators=750, pca__n_components=300\n",
      "[CV] rf__n_estimators=750, pca__n_components=300, score=0.64023  -  43.8s\n",
      "[CV] rf__n_estimators=750, pca__n_components=300\n",
      "[CV] rf__n_estimators=750, pca__n_components=300, score=0.66162  -  43.3s\n",
      "[CV] rf__n_estimators=750, pca__n_components=300\n",
      "[CV] rf__n_estimators=750, pca__n_components=300, score=0.62970  -  43.5s\n",
      "[CV] rf__n_estimators=750, pca__n_components=300\n",
      "[CV] rf__n_estimators=750, pca__n_components=300, score=0.68356  -  43.5s\n",
      "[CV] rf__n_estimators=750, pca__n_components=300\n",
      "[CV] rf__n_estimators=750, pca__n_components=300, score=0.62106  -  43.4s\n",
      "Dict{Symbol, Any}(:rf__n_estimators => 100, :pca__n_components => 200)\n"
     ]
    }
   ],
   "source": [
    "# do hyperparameter tuning\n",
    "rocauc = make_scorer(roc_auc_score)\n",
    "xgb_tuned = GridSearchCV(pipe, params_pipe, cv=cv, scoring=rocauc, verbose=3)\n",
    "ScikitLearn.fit!(xgb_tuned, X_train, y_train)\n",
    "println(xgb_tuned.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3a683ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8074007391263148"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test set evaluation\n",
    "rf = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\")\n",
    "pca = PCA(n_components=200)\n",
    "pipe = Pipeline([(\"pca\", pca), (\"rf\", rf)])\n",
    "model = ScikitLearn.fit!(pipe, X_train, y_train)\n",
    "scores_pos = ScikitLearn.predict_proba(model, X_test)[:,2]\n",
    "roc_auc_score(y_test, scores_pos)\n",
    "#auc_from_scores(scores_flat, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740bd8d1",
   "metadata": {},
   "source": [
    "##### Concatenating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6fd6e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle and set test set aside\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_b, labels, test_size=0.2, stratify=labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ed3e5aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model and CV\n",
    "rf = RandomForestClassifier(class_weight=\"balanced\")\n",
    "pca = PCA()\n",
    "pipe = Pipeline([(\"pca\", pca), (\"rf\", rf)])\n",
    "params_pipe = Dict(\"pca__n_components\"=>[50, 100, 200, 300], \"rf__n_estimators\"=>[100, 250, 500, 750])\n",
    "cv = CrossValidation.StratifiedKFold(y_train, n_folds=5, shuffle=true, random_state=42);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8cf26717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] rf__n_estimators=100, pca__n_components=50\n",
      "[CV] rf__n_estimators=100, pca__n_components=50, score=0.63925  -  5.0s\n",
      "[CV] rf__n_estimators=100, pca__n_components=50\n",
      "[CV] rf__n_estimators=100, pca__n_components=50, score=0.59941  -  4.9s\n",
      "[CV] rf__n_estimators=100, pca__n_components=50\n",
      "[CV] rf__n_estimators=100, pca__n_components=50, score=0.61982  -  4.8s\n",
      "[CV] rf__n_estimators=100, pca__n_components=50\n",
      "[CV] rf__n_estimators=100, pca__n_components=50, score=0.61163  -  4.7s\n",
      "[CV] rf__n_estimators=100, pca__n_components=50\n",
      "[CV] rf__n_estimators=100, pca__n_components=50, score=0.68454  -  4.9s\n",
      "[CV] rf__n_estimators=250, pca__n_components=50\n",
      "[CV] rf__n_estimators=250, pca__n_components=50, score=0.63925  -  7.5s\n",
      "[CV] rf__n_estimators=250, pca__n_components=50\n",
      "[CV] rf__n_estimators=250, pca__n_components=50, score=0.59941  -  7.3s\n",
      "[CV] rf__n_estimators=250, pca__n_components=50\n",
      "[CV] rf__n_estimators=250, pca__n_components=50, score=0.63068  -  7.5s\n",
      "[CV] rf__n_estimators=250, pca__n_components=50\n",
      "[CV] rf__n_estimators=250, pca__n_components=50, score=0.61163  -  7.3s\n",
      "[CV] rf__n_estimators=250, pca__n_components=50\n",
      "[CV] rf__n_estimators=250, pca__n_components=50, score=0.69496  -  7.5s\n",
      "[CV] rf__n_estimators=500, pca__n_components=50\n",
      "[CV] rf__n_estimators=500, pca__n_components=50, score=0.63925  -  11.7s\n",
      "[CV] rf__n_estimators=500, pca__n_components=50\n",
      "[CV] rf__n_estimators=500, pca__n_components=50, score=0.58921  -  11.8s\n",
      "[CV] rf__n_estimators=500, pca__n_components=50\n",
      "[CV] rf__n_estimators=500, pca__n_components=50, score=0.63101  -  11.7s\n",
      "[CV] rf__n_estimators=500, pca__n_components=50\n",
      "[CV] rf__n_estimators=500, pca__n_components=50, score=0.62204  -  11.9s\n",
      "[CV] rf__n_estimators=500, pca__n_components=50\n",
      "[CV] rf__n_estimators=500, pca__n_components=50, score=0.67413  -  11.8s\n",
      "[CV] rf__n_estimators=750, pca__n_components=50\n",
      "[CV] rf__n_estimators=750, pca__n_components=50, score=0.63925  -  16.1s\n",
      "[CV] rf__n_estimators=750, pca__n_components=50\n",
      "[CV] rf__n_estimators=750, pca__n_components=50, score=0.58888  -  15.9s\n",
      "[CV] rf__n_estimators=750, pca__n_components=50\n",
      "[CV] rf__n_estimators=750, pca__n_components=50, score=0.63068  -  16.2s\n",
      "[CV] rf__n_estimators=750, pca__n_components=50\n",
      "[CV] rf__n_estimators=750, pca__n_components=50, score=0.61163  -  15.8s\n",
      "[CV] rf__n_estimators=750, pca__n_components=50\n",
      "[CV] rf__n_estimators=750, pca__n_components=50, score=0.68454  -  16.4s\n",
      "[CV] rf__n_estimators=100, pca__n_components=100\n",
      "[CV] rf__n_estimators=100, pca__n_components=100, score=0.63925  -  7.3s\n",
      "[CV] rf__n_estimators=100, pca__n_components=100\n",
      "[CV] rf__n_estimators=100, pca__n_components=100, score=0.63003  -  7.5s\n",
      "[CV] rf__n_estimators=100, pca__n_components=100\n",
      "[CV] rf__n_estimators=100, pca__n_components=100, score=0.65142  -  7.3s\n",
      "[CV] rf__n_estimators=100, pca__n_components=100\n",
      "[CV] rf__n_estimators=100, pca__n_components=100, score=0.61163  -  7.6s\n",
      "[CV] rf__n_estimators=100, pca__n_components=100\n",
      "[CV] rf__n_estimators=100, pca__n_components=100, score=0.68454  -  7.6s\n",
      "[CV] rf__n_estimators=250, pca__n_components=100\n",
      "[CV] rf__n_estimators=250, pca__n_components=100, score=0.63925  -  11.4s\n",
      "[CV] rf__n_estimators=250, pca__n_components=100\n",
      "[CV] rf__n_estimators=250, pca__n_components=100, score=0.63003  -  11.2s\n",
      "[CV] rf__n_estimators=250, pca__n_components=100\n",
      "[CV] rf__n_estimators=250, pca__n_components=100, score=0.66162  -  11.2s\n",
      "[CV] rf__n_estimators=250, pca__n_components=100\n",
      "[CV] rf__n_estimators=250, pca__n_components=100, score=0.61163  -  11.9s\n",
      "[CV] rf__n_estimators=250, pca__n_components=100\n",
      "[CV] rf__n_estimators=250, pca__n_components=100, score=0.68454  -  12.4s\n",
      "[CV] rf__n_estimators=500, pca__n_components=100\n",
      "[CV] rf__n_estimators=500, pca__n_components=100, score=0.63925  -  17.5s\n",
      "[CV] rf__n_estimators=500, pca__n_components=100\n",
      "[CV] rf__n_estimators=500, pca__n_components=100, score=0.63003  -  19.4s\n",
      "[CV] rf__n_estimators=500, pca__n_components=100\n",
      "[CV] rf__n_estimators=500, pca__n_components=100, score=0.65142  -  17.6s\n",
      "[CV] rf__n_estimators=500, pca__n_components=100\n",
      "[CV] rf__n_estimators=500, pca__n_components=100, score=0.61163  -  17.8s\n",
      "[CV] rf__n_estimators=500, pca__n_components=100\n",
      "[CV] rf__n_estimators=500, pca__n_components=100, score=0.68454  -  17.8s\n",
      "[CV] rf__n_estimators=750, pca__n_components=100\n",
      "[CV] rf__n_estimators=750, pca__n_components=100, score=0.63925  -  24.5s\n",
      "[CV] rf__n_estimators=750, pca__n_components=100\n",
      "[CV] rf__n_estimators=750, pca__n_components=100, score=0.63003  -  24.0s\n",
      "[CV] rf__n_estimators=750, pca__n_components=100\n",
      "[CV] rf__n_estimators=750, pca__n_components=100, score=0.66162  -  23.2s\n",
      "[CV] rf__n_estimators=750, pca__n_components=100\n",
      "[CV] rf__n_estimators=750, pca__n_components=100, score=0.61163  -  23.7s\n",
      "[CV] rf__n_estimators=750, pca__n_components=100\n",
      "[CV] rf__n_estimators=750, pca__n_components=100, score=0.68454  -  25.1s\n",
      "[CV] rf__n_estimators=100, pca__n_components=200\n",
      "[CV] rf__n_estimators=100, pca__n_components=200, score=0.63925  -  12.3s\n",
      "[CV] rf__n_estimators=100, pca__n_components=200\n",
      "[CV] rf__n_estimators=100, pca__n_components=200, score=0.65011  -  12.2s\n",
      "[CV] rf__n_estimators=100, pca__n_components=200\n",
      "[CV] rf__n_estimators=100, pca__n_components=200, score=0.65142  -  12.6s\n",
      "[CV] rf__n_estimators=100, pca__n_components=200\n",
      "[CV] rf__n_estimators=100, pca__n_components=200, score=0.61163  -  12.5s\n",
      "[CV] rf__n_estimators=100, pca__n_components=200\n",
      "[CV] rf__n_estimators=100, pca__n_components=200, score=0.68454  -  12.6s\n",
      "[CV] rf__n_estimators=250, pca__n_components=200\n",
      "[CV] rf__n_estimators=250, pca__n_components=200, score=0.63925  -  18.2s\n",
      "[CV] rf__n_estimators=250, pca__n_components=200\n",
      "[CV] rf__n_estimators=250, pca__n_components=200, score=0.61982  -  17.6s\n",
      "[CV] rf__n_estimators=250, pca__n_components=200\n",
      "[CV] rf__n_estimators=250, pca__n_components=200, score=0.65142  -  17.4s\n",
      "[CV] rf__n_estimators=250, pca__n_components=200\n",
      "[CV] rf__n_estimators=250, pca__n_components=200, score=0.61163  -  17.7s\n",
      "[CV] rf__n_estimators=250, pca__n_components=200\n",
      "[CV] rf__n_estimators=250, pca__n_components=200, score=0.67413  -  17.9s\n",
      "[CV] rf__n_estimators=500, pca__n_components=200\n",
      "[CV] rf__n_estimators=500, pca__n_components=200, score=0.63925  -  25.9s\n",
      "[CV] rf__n_estimators=500, pca__n_components=200\n",
      "[CV] rf__n_estimators=500, pca__n_components=200, score=0.64023  -  25.7s\n",
      "[CV] rf__n_estimators=500, pca__n_components=200\n",
      "[CV] rf__n_estimators=500, pca__n_components=200, score=0.65142  -  25.5s\n",
      "[CV] rf__n_estimators=500, pca__n_components=200\n",
      "[CV] rf__n_estimators=500, pca__n_components=200, score=0.61163  -  25.7s\n",
      "[CV] rf__n_estimators=500, pca__n_components=200\n",
      "[CV] rf__n_estimators=500, pca__n_components=200, score=0.67413  -  26.0s\n",
      "[CV] rf__n_estimators=750, pca__n_components=200\n",
      "[CV] rf__n_estimators=750, pca__n_components=200, score=0.63925  -  34.6s\n",
      "[CV] rf__n_estimators=750, pca__n_components=200\n",
      "[CV] rf__n_estimators=750, pca__n_components=200, score=0.64023  -  34.7s\n",
      "[CV] rf__n_estimators=750, pca__n_components=200\n",
      "[CV] rf__n_estimators=750, pca__n_components=200, score=0.65142  -  34.0s\n",
      "[CV] rf__n_estimators=750, pca__n_components=200\n",
      "[CV] rf__n_estimators=750, pca__n_components=200, score=0.61163  -  33.6s\n",
      "[CV] rf__n_estimators=750, pca__n_components=200\n",
      "[CV] rf__n_estimators=750, pca__n_components=200, score=0.68454  -  34.3s\n",
      "[CV] rf__n_estimators=100, pca__n_components=300\n",
      "[CV] rf__n_estimators=100, pca__n_components=300, score=0.63925  -  16.2s\n",
      "[CV] rf__n_estimators=100, pca__n_components=300\n",
      "[CV] rf__n_estimators=100, pca__n_components=300, score=0.63003  -  16.4s\n",
      "[CV] rf__n_estimators=100, pca__n_components=300\n",
      "[CV] rf__n_estimators=100, pca__n_components=300, score=0.64122  -  16.9s\n",
      "[CV] rf__n_estimators=100, pca__n_components=300\n",
      "[CV] rf__n_estimators=100, pca__n_components=300, score=0.61196  -  17.2s\n",
      "[CV] rf__n_estimators=100, pca__n_components=300\n",
      "[CV] rf__n_estimators=100, pca__n_components=300, score=0.68454  -  17.3s\n",
      "[CV] rf__n_estimators=250, pca__n_components=300\n",
      "[CV] rf__n_estimators=250, pca__n_components=300, score=0.63925  -  24.3s\n",
      "[CV] rf__n_estimators=250, pca__n_components=300\n",
      "[CV] rf__n_estimators=250, pca__n_components=300, score=0.63003  -  23.5s\n",
      "[CV] rf__n_estimators=250, pca__n_components=300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] rf__n_estimators=250, pca__n_components=300, score=0.65142  -  22.6s\n",
      "[CV] rf__n_estimators=250, pca__n_components=300\n",
      "[CV] rf__n_estimators=250, pca__n_components=300, score=0.61163  -  24.2s\n",
      "[CV] rf__n_estimators=250, pca__n_components=300\n",
      "[CV] rf__n_estimators=250, pca__n_components=300, score=0.68454  -  23.5s\n",
      "[CV] rf__n_estimators=500, pca__n_components=300\n",
      "[CV] rf__n_estimators=500, pca__n_components=300, score=0.63925  -  33.8s\n",
      "[CV] rf__n_estimators=500, pca__n_components=300\n",
      "[CV] rf__n_estimators=500, pca__n_components=300, score=0.61982  -  34.5s\n",
      "[CV] rf__n_estimators=500, pca__n_components=300\n",
      "[CV] rf__n_estimators=500, pca__n_components=300, score=0.65142  -  32.7s\n",
      "[CV] rf__n_estimators=500, pca__n_components=300\n",
      "[CV] rf__n_estimators=500, pca__n_components=300, score=0.61163  -  33.1s\n",
      "[CV] rf__n_estimators=500, pca__n_components=300\n",
      "[CV] rf__n_estimators=500, pca__n_components=300, score=0.68454  -  34.2s\n",
      "[CV] rf__n_estimators=750, pca__n_components=300\n",
      "[CV] rf__n_estimators=750, pca__n_components=300, score=0.63925  -  48.5s\n",
      "[CV] rf__n_estimators=750, pca__n_components=300\n",
      "[CV] rf__n_estimators=750, pca__n_components=300, score=0.63003  -  45.7s\n",
      "[CV] rf__n_estimators=750, pca__n_components=300\n",
      "[CV] rf__n_estimators=750, pca__n_components=300, score=0.65142  -  44.0s\n",
      "[CV] rf__n_estimators=750, pca__n_components=300\n",
      "[CV] rf__n_estimators=750, pca__n_components=300, score=0.61163  -  43.0s\n",
      "[CV] rf__n_estimators=750, pca__n_components=300\n",
      "[CV] rf__n_estimators=750, pca__n_components=300, score=0.68454  -  43.4s\n",
      "Dict{Symbol, Any}(:rf__n_estimators => 100, :pca__n_components => 200)\n"
     ]
    }
   ],
   "source": [
    "# do hyperparameter tuning\n",
    "rocauc = make_scorer(roc_auc_score)\n",
    "xgb_tuned = GridSearchCV(pipe, params_pipe, cv=cv, scoring=rocauc, verbose=3)\n",
    "ScikitLearn.fit!(xgb_tuned, X_train, y_train)\n",
    "println(xgb_tuned.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4ff156cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8164933711223865"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test set evaluation\n",
    "rf = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\")\n",
    "pca = PCA(n_components=200)\n",
    "pipe = Pipeline([(\"pca\", pca), (\"rf\", rf)])\n",
    "model = ScikitLearn.fit!(pipe, X_train, y_train)\n",
    "scores_pos = ScikitLearn.predict_proba(model, X_test)[:,2]\n",
    "roc_auc_score(y_test, scores_pos)\n",
    "#auc_from_scores(scores_flat, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "72e647af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8851564585500992\n"
     ]
    }
   ],
   "source": [
    "# doublecheck the other way around!\n",
    "rf = RandomForestClassifier(n_estimators=500, class_weight=\"balanced\")\n",
    "pca = PCA(n_components=300)\n",
    "pipe = Pipeline([(\"pca\", pca), (\"rf\", rf)])\n",
    "auc_scores = []\n",
    "\n",
    "for (train_indices, test_indices) in CrossValidation.StratifiedKFold(labels, n_folds=5, shuffle=true, random_state=42)\n",
    "    # define training and test data\n",
    "    x_train = features_c[train_indices, :]\n",
    "    y_train = labels[train_indices]\n",
    "    x_test = features_c[test_indices, :]\n",
    "    y_test = labels[test_indices]\n",
    "    \n",
    "    # train the model\n",
    "    model = ScikitLearn.fit!(pipe, x_train, y_train)\n",
    "    \n",
    "    # make predictions and keep the scores\n",
    "    scores_pos = ScikitLearn.predict_proba(model, x_test)[:,2]\n",
    "    push!(auc_scores, roc_auc_score(y_test, scores_pos))\n",
    "end\n",
    "println(mean(auc_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45824cf2",
   "metadata": {},
   "source": [
    "## 6 - Do PCA and build XGBoost model<a name=\"machinelearningxgb\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaee0428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model and CV\n",
    "xgb = xgboost(x_train_r, 500, label=y_train, scale_pos_weight=1/imbalance)\n",
    "\n",
    "\n",
    "pca = PCA()\n",
    "pipe = Pipeline([(\"pca\", pca), (\"logistic\", logistic)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31680e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imbalance = length(signatures_pos)/length(signatures_neg);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee199089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the cross-validation and keep the scores\n",
    "similarities = zeros(Float64, length(loci_embeddings), length(rbp_embeddings))\n",
    "scores_flat = zeros(Float64, length(signatures))\n",
    "\n",
    "for (train_indices, test_indices) in CrossValidation.StratifiedKFold(labels, n_folds=10, \n",
    "        shuffle=true, random_state=42)\n",
    "    # define training and test data\n",
    "    x_train = signatures_matrix[train_indices, :]\n",
    "    y_train = labels[train_indices]\n",
    "    x_test = signatures_matrix[test_indices, :]\n",
    "    y_test = labels[test_indices]\n",
    "    \n",
    "    # train the model\n",
    "    pca = PCA(n_components=300)\n",
    "    x_train_r = pca.fit_transform(x_train)\n",
    "    model = xgboost(x_train_r, 500, label=y_train, scale_pos_weight=1/imbalance)\n",
    "    \n",
    "    # make predictions and keep the scores\n",
    "    x_test_r = pca.transform(x_test)\n",
    "    scores_pos = XGBoost.predict(model, x_test_r)\n",
    "    \n",
    "    for (i, test_i) in enumerate(test_indices)\n",
    "        pos_i, pos_j = positions[test_i][1], positions[test_i][2]\n",
    "        similarities[pos_i, pos_j] = scores_pos[i]\n",
    "        scores_flat[test_i] = scores_pos[i]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb0275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_from_scores(scores_flat, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d4e0e8",
   "metadata": {},
   "source": [
    "## 6 - HDC learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12732985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert dictionaries to arrays for training\n",
    "sign_pos_list = [value for (key, value) in signatures_pos]\n",
    "sign_neg_list = [value for (key, value) in signatures_neg]\n",
    "signatures_pos_array = convert(Array{BipolarHDV}, sign_pos_list)\n",
    "signatures_neg_array = convert(Array{BipolarHDV}, sign_neg_list);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9e7ed8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle and split in train-test\n",
    "sign_pos = shuffle(signatures_pos_array)\n",
    "sign_neg = shuffle(signatures_neg_array)\n",
    "cutoff_pos = Int(round(length(signatures_pos_array)*0.75))\n",
    "cutoff_neg = Int(round(length(signatures_neg_array)*0.75))\n",
    "training_pos = sign_pos[1:cutoff_pos]\n",
    "training_neg = sign_neg[1:cutoff_neg]\n",
    "testing_pos = sign_pos[cutoff_pos+1:end]\n",
    "testing_neg = sign_neg[cutoff_neg+1:end]\n",
    "\n",
    "# train classes (aggregate)\n",
    "training_pos_agg = HyperdimensionalComputing.aggregate(training_pos)\n",
    "training_neg_agg = HyperdimensionalComputing.aggregate(training_neg);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff4552f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute distances (Cosine sim for RealHDVs)\n",
    "preds_pos = [cos_sim(training_pos_agg, x) > cos_sim(training_neg_agg, x) ? 1 : 0 for x in testing_pos]\n",
    "preds_neg = [cos_sim(training_pos_agg, x) > cos_sim(training_neg_agg, x) ? 1 : 0 for x in testing_neg];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57bb9fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02258064516129032"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (sum(preds_pos .== 1) + sum(preds_neg .== 0)) / (length(testing_pos)+length(testing_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b900da12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip270\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip270)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip271\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip270)\" d=\"\n",
       "M279.704 1423.18 L2352.76 1423.18 L2352.76 47.2441 L279.704 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip272\">\n",
       "    <rect x=\"279\" y=\"47\" width=\"2074\" height=\"1377\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip272)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  331.899,1423.18 331.899,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip272)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  979.485,1423.18 979.485,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip272)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1627.07,1423.18 1627.07,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip272)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2274.66,1423.18 2274.66,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip270)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  279.704,1423.18 2352.76,1423.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip270)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  331.899,1423.18 331.899,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip270)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  979.485,1423.18 979.485,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip270)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1627.07,1423.18 1627.07,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip270)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2274.66,1423.18 2274.66,1404.28 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip270)\" d=\"M331.899 1454.1 Q328.288 1454.1 326.46 1457.66 Q324.654 1461.2 324.654 1468.33 Q324.654 1475.44 326.46 1479.01 Q328.288 1482.55 331.899 1482.55 Q335.534 1482.55 337.339 1479.01 Q339.168 1475.44 339.168 1468.33 Q339.168 1461.2 337.339 1457.66 Q335.534 1454.1 331.899 1454.1 M331.899 1450.39 Q337.709 1450.39 340.765 1455 Q343.844 1459.58 343.844 1468.33 Q343.844 1477.06 340.765 1481.67 Q337.709 1486.25 331.899 1486.25 Q326.089 1486.25 323.01 1481.67 Q319.955 1477.06 319.955 1468.33 Q319.955 1459.58 323.01 1455 Q326.089 1450.39 331.899 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M939.092 1481.64 L946.731 1481.64 L946.731 1455.28 L938.421 1456.95 L938.421 1452.69 L946.684 1451.02 L951.36 1451.02 L951.36 1481.64 L958.999 1481.64 L958.999 1485.58 L939.092 1485.58 L939.092 1481.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M978.444 1454.1 Q974.832 1454.1 973.004 1457.66 Q971.198 1461.2 971.198 1468.33 Q971.198 1475.44 973.004 1479.01 Q974.832 1482.55 978.444 1482.55 Q982.078 1482.55 983.883 1479.01 Q985.712 1475.44 985.712 1468.33 Q985.712 1461.2 983.883 1457.66 Q982.078 1454.1 978.444 1454.1 M978.444 1450.39 Q984.254 1450.39 987.309 1455 Q990.388 1459.58 990.388 1468.33 Q990.388 1477.06 987.309 1481.67 Q984.254 1486.25 978.444 1486.25 Q972.633 1486.25 969.555 1481.67 Q966.499 1477.06 966.499 1468.33 Q966.499 1459.58 969.555 1455 Q972.633 1450.39 978.444 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M1008.61 1454.1 Q1004.99 1454.1 1003.17 1457.66 Q1001.36 1461.2 1001.36 1468.33 Q1001.36 1475.44 1003.17 1479.01 Q1004.99 1482.55 1008.61 1482.55 Q1012.24 1482.55 1014.05 1479.01 Q1015.87 1475.44 1015.87 1468.33 Q1015.87 1461.2 1014.05 1457.66 Q1012.24 1454.1 1008.61 1454.1 M1008.61 1450.39 Q1014.42 1450.39 1017.47 1455 Q1020.55 1459.58 1020.55 1468.33 Q1020.55 1477.06 1017.47 1481.67 Q1014.42 1486.25 1008.61 1486.25 Q1002.8 1486.25 999.717 1481.67 Q996.661 1477.06 996.661 1468.33 Q996.661 1459.58 999.717 1455 Q1002.8 1450.39 1008.61 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M1590.76 1481.64 L1607.08 1481.64 L1607.08 1485.58 L1585.14 1485.58 L1585.14 1481.64 Q1587.8 1478.89 1592.38 1474.26 Q1596.99 1469.61 1598.17 1468.27 Q1600.42 1465.74 1601.3 1464.01 Q1602.2 1462.25 1602.2 1460.56 Q1602.2 1457.8 1600.25 1456.07 Q1598.33 1454.33 1595.23 1454.33 Q1593.03 1454.33 1590.58 1455.09 Q1588.15 1455.86 1585.37 1457.41 L1585.37 1452.69 Q1588.19 1451.55 1590.65 1450.97 Q1593.1 1450.39 1595.14 1450.39 Q1600.51 1450.39 1603.7 1453.08 Q1606.9 1455.77 1606.9 1460.26 Q1606.9 1462.39 1606.09 1464.31 Q1605.3 1466.2 1603.19 1468.8 Q1602.62 1469.47 1599.51 1472.69 Q1596.41 1475.88 1590.76 1481.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M1626.9 1454.1 Q1623.29 1454.1 1621.46 1457.66 Q1619.65 1461.2 1619.65 1468.33 Q1619.65 1475.44 1621.46 1479.01 Q1623.29 1482.55 1626.9 1482.55 Q1630.53 1482.55 1632.34 1479.01 Q1634.17 1475.44 1634.17 1468.33 Q1634.17 1461.2 1632.34 1457.66 Q1630.53 1454.1 1626.9 1454.1 M1626.9 1450.39 Q1632.71 1450.39 1635.76 1455 Q1638.84 1459.58 1638.84 1468.33 Q1638.84 1477.06 1635.76 1481.67 Q1632.71 1486.25 1626.9 1486.25 Q1621.09 1486.25 1618.01 1481.67 Q1614.95 1477.06 1614.95 1468.33 Q1614.95 1459.58 1618.01 1455 Q1621.09 1450.39 1626.9 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M1657.06 1454.1 Q1653.45 1454.1 1651.62 1457.66 Q1649.81 1461.2 1649.81 1468.33 Q1649.81 1475.44 1651.62 1479.01 Q1653.45 1482.55 1657.06 1482.55 Q1660.69 1482.55 1662.5 1479.01 Q1664.33 1475.44 1664.33 1468.33 Q1664.33 1461.2 1662.5 1457.66 Q1660.69 1454.1 1657.06 1454.1 M1657.06 1450.39 Q1662.87 1450.39 1665.93 1455 Q1669 1459.58 1669 1468.33 Q1669 1477.06 1665.93 1481.67 Q1662.87 1486.25 1657.06 1486.25 Q1651.25 1486.25 1648.17 1481.67 Q1645.12 1477.06 1645.12 1468.33 Q1645.12 1459.58 1648.17 1455 Q1651.25 1450.39 1657.06 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M2248.42 1466.95 Q2251.78 1467.66 2253.65 1469.93 Q2255.55 1472.2 2255.55 1475.53 Q2255.55 1480.65 2252.03 1483.45 Q2248.51 1486.25 2242.03 1486.25 Q2239.85 1486.25 2237.54 1485.81 Q2235.25 1485.39 2232.79 1484.54 L2232.79 1480.02 Q2234.74 1481.16 2237.05 1481.74 Q2239.37 1482.32 2241.89 1482.32 Q2246.29 1482.32 2248.58 1480.58 Q2250.9 1478.84 2250.9 1475.53 Q2250.9 1472.48 2248.74 1470.77 Q2246.61 1469.03 2242.79 1469.03 L2238.77 1469.03 L2238.77 1465.19 L2242.98 1465.19 Q2246.43 1465.19 2248.26 1463.82 Q2250.09 1462.43 2250.09 1459.84 Q2250.09 1457.18 2248.19 1455.77 Q2246.31 1454.33 2242.79 1454.33 Q2240.87 1454.33 2238.67 1454.75 Q2236.47 1455.16 2233.84 1456.04 L2233.84 1451.88 Q2236.5 1451.14 2238.81 1450.77 Q2241.15 1450.39 2243.21 1450.39 Q2248.53 1450.39 2251.64 1452.83 Q2254.74 1455.23 2254.74 1459.35 Q2254.74 1462.22 2253.09 1464.21 Q2251.45 1466.18 2248.42 1466.95 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M2274.41 1454.1 Q2270.8 1454.1 2268.97 1457.66 Q2267.17 1461.2 2267.17 1468.33 Q2267.17 1475.44 2268.97 1479.01 Q2270.8 1482.55 2274.41 1482.55 Q2278.05 1482.55 2279.85 1479.01 Q2281.68 1475.44 2281.68 1468.33 Q2281.68 1461.2 2279.85 1457.66 Q2278.05 1454.1 2274.41 1454.1 M2274.41 1450.39 Q2280.22 1450.39 2283.28 1455 Q2286.36 1459.58 2286.36 1468.33 Q2286.36 1477.06 2283.28 1481.67 Q2280.22 1486.25 2274.41 1486.25 Q2268.6 1486.25 2265.53 1481.67 Q2262.47 1477.06 2262.47 1468.33 Q2262.47 1459.58 2265.53 1455 Q2268.6 1450.39 2274.41 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M2304.58 1454.1 Q2300.96 1454.1 2299.14 1457.66 Q2297.33 1461.2 2297.33 1468.33 Q2297.33 1475.44 2299.14 1479.01 Q2300.96 1482.55 2304.58 1482.55 Q2308.21 1482.55 2310.02 1479.01 Q2311.84 1475.44 2311.84 1468.33 Q2311.84 1461.2 2310.02 1457.66 Q2308.21 1454.1 2304.58 1454.1 M2304.58 1450.39 Q2310.39 1450.39 2313.44 1455 Q2316.52 1459.58 2316.52 1468.33 Q2316.52 1477.06 2313.44 1481.67 Q2310.39 1486.25 2304.58 1486.25 Q2298.77 1486.25 2295.69 1481.67 Q2292.63 1477.06 2292.63 1468.33 Q2292.63 1459.58 2295.69 1455 Q2298.77 1450.39 2304.58 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M1267.26 1537.87 Q1266.28 1537.3 1265.1 1537.04 Q1263.95 1536.76 1262.55 1536.76 Q1257.59 1536.76 1254.91 1540 Q1252.27 1543.22 1252.27 1549.27 L1252.27 1568.04 L1246.38 1568.04 L1246.38 1532.4 L1252.27 1532.4 L1252.27 1537.93 Q1254.12 1534.69 1257.08 1533.13 Q1260.04 1531.54 1264.27 1531.54 Q1264.87 1531.54 1265.61 1531.63 Q1266.34 1531.7 1267.23 1531.85 L1267.26 1537.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M1289.61 1550.12 Q1282.51 1550.12 1279.77 1551.75 Q1277.03 1553.37 1277.03 1557.29 Q1277.03 1560.4 1279.07 1562.25 Q1281.14 1564.07 1284.67 1564.07 Q1289.54 1564.07 1292.47 1560.63 Q1295.43 1557.16 1295.43 1551.43 L1295.43 1550.12 L1289.61 1550.12 M1301.29 1547.71 L1301.29 1568.04 L1295.43 1568.04 L1295.43 1562.63 Q1293.42 1565.88 1290.43 1567.44 Q1287.44 1568.97 1283.11 1568.97 Q1277.64 1568.97 1274.39 1565.91 Q1271.18 1562.82 1271.18 1557.67 Q1271.18 1551.65 1275.19 1548.6 Q1279.23 1545.54 1287.22 1545.54 L1295.43 1545.54 L1295.43 1544.97 Q1295.43 1540.93 1292.76 1538.73 Q1290.11 1536.5 1285.31 1536.5 Q1282.25 1536.5 1279.36 1537.23 Q1276.46 1537.97 1273.79 1539.43 L1273.79 1534.02 Q1277 1532.78 1280.02 1532.17 Q1283.05 1531.54 1285.91 1531.54 Q1293.65 1531.54 1297.47 1535.55 Q1301.29 1539.56 1301.29 1547.71 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M1342.98 1546.53 L1342.98 1568.04 L1337.13 1568.04 L1337.13 1546.72 Q1337.13 1541.66 1335.15 1539.14 Q1333.18 1536.63 1329.23 1536.63 Q1324.49 1536.63 1321.75 1539.65 Q1319.01 1542.68 1319.01 1547.9 L1319.01 1568.04 L1313.13 1568.04 L1313.13 1532.4 L1319.01 1532.4 L1319.01 1537.93 Q1321.12 1534.72 1323.95 1533.13 Q1326.81 1531.54 1330.54 1531.54 Q1336.68 1531.54 1339.83 1535.36 Q1342.98 1539.14 1342.98 1546.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M1354.44 1518.52 L1360.33 1518.52 L1360.33 1547.77 L1377.8 1532.4 L1385.28 1532.4 L1366.38 1549.07 L1386.08 1568.04 L1378.44 1568.04 L1360.33 1550.63 L1360.33 1568.04 L1354.44 1568.04 L1354.44 1518.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip272)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  279.704,1413.29 2352.76,1413.29 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip272)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  279.704,1190.6 2352.76,1190.6 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip272)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  279.704,967.905 2352.76,967.905 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip272)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  279.704,745.213 2352.76,745.213 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip272)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  279.704,522.521 2352.76,522.521 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip272)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  279.704,299.829 2352.76,299.829 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip272)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  279.704,77.1364 2352.76,77.1364 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip270)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  279.704,1423.18 279.704,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip270)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  279.704,1413.29 298.602,1413.29 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip270)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  279.704,1190.6 298.602,1190.6 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip270)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  279.704,967.905 298.602,967.905 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip270)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  279.704,745.213 298.602,745.213 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip270)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  279.704,522.521 298.602,522.521 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip270)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  279.704,299.829 298.602,299.829 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip270)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  279.704,77.1364 298.602,77.1364 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip270)\" d=\"M127.2 1399.09 Q123.589 1399.09 121.76 1402.65 Q119.955 1406.19 119.955 1413.32 Q119.955 1420.43 121.76 1424 Q123.589 1427.54 127.2 1427.54 Q130.834 1427.54 132.64 1424 Q134.468 1420.43 134.468 1413.32 Q134.468 1406.19 132.64 1402.65 Q130.834 1399.09 127.2 1399.09 M127.2 1395.38 Q133.01 1395.38 136.066 1399.99 Q139.144 1404.57 139.144 1413.32 Q139.144 1422.05 136.066 1426.66 Q133.01 1431.24 127.2 1431.24 Q121.39 1431.24 118.311 1426.66 Q115.256 1422.05 115.256 1413.32 Q115.256 1404.57 118.311 1399.99 Q121.39 1395.38 127.2 1395.38 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M147.362 1424.69 L152.246 1424.69 L152.246 1430.57 L147.362 1430.57 L147.362 1424.69 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M172.431 1399.09 Q168.82 1399.09 166.991 1402.65 Q165.186 1406.19 165.186 1413.32 Q165.186 1420.43 166.991 1424 Q168.82 1427.54 172.431 1427.54 Q176.065 1427.54 177.871 1424 Q179.7 1420.43 179.7 1413.32 Q179.7 1406.19 177.871 1402.65 Q176.065 1399.09 172.431 1399.09 M172.431 1395.38 Q178.241 1395.38 181.297 1399.99 Q184.376 1404.57 184.376 1413.32 Q184.376 1422.05 181.297 1426.66 Q178.241 1431.24 172.431 1431.24 Q166.621 1431.24 163.542 1426.66 Q160.487 1422.05 160.487 1413.32 Q160.487 1404.57 163.542 1399.99 Q166.621 1395.38 172.431 1395.38 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M202.593 1399.09 Q198.982 1399.09 197.153 1402.65 Q195.348 1406.19 195.348 1413.32 Q195.348 1420.43 197.153 1424 Q198.982 1427.54 202.593 1427.54 Q206.227 1427.54 208.033 1424 Q209.861 1420.43 209.861 1413.32 Q209.861 1406.19 208.033 1402.65 Q206.227 1399.09 202.593 1399.09 M202.593 1395.38 Q208.403 1395.38 211.459 1399.99 Q214.537 1404.57 214.537 1413.32 Q214.537 1422.05 211.459 1426.66 Q208.403 1431.24 202.593 1431.24 Q196.783 1431.24 193.704 1426.66 Q190.649 1422.05 190.649 1413.32 Q190.649 1404.57 193.704 1399.99 Q196.783 1395.38 202.593 1395.38 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M222.801 1396.01 L241.158 1396.01 L241.158 1399.94 L227.084 1399.94 L227.084 1408.42 Q228.102 1408.07 229.121 1407.91 Q230.139 1407.72 231.158 1407.72 Q236.945 1407.72 240.324 1410.89 Q243.704 1414.06 243.704 1419.48 Q243.704 1425.06 240.232 1428.16 Q236.76 1431.24 230.44 1431.24 Q228.264 1431.24 225.996 1430.87 Q223.75 1430.5 221.343 1429.76 L221.343 1425.06 Q223.426 1426.19 225.648 1426.75 Q227.871 1427.31 230.347 1427.31 Q234.352 1427.31 236.69 1425.2 Q239.028 1423.09 239.028 1419.48 Q239.028 1415.87 236.69 1413.76 Q234.352 1411.66 230.347 1411.66 Q228.473 1411.66 226.598 1412.07 Q224.746 1412.49 222.801 1413.37 L222.801 1396.01 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M126.205 1176.4 Q122.593 1176.4 120.765 1179.96 Q118.959 1183.5 118.959 1190.63 Q118.959 1197.74 120.765 1201.3 Q122.593 1204.84 126.205 1204.84 Q129.839 1204.84 131.644 1201.3 Q133.473 1197.74 133.473 1190.63 Q133.473 1183.5 131.644 1179.96 Q129.839 1176.4 126.205 1176.4 M126.205 1172.69 Q132.015 1172.69 135.07 1177.3 Q138.149 1181.88 138.149 1190.63 Q138.149 1199.36 135.07 1203.97 Q132.015 1208.55 126.205 1208.55 Q120.394 1208.55 117.316 1203.97 Q114.26 1199.36 114.26 1190.63 Q114.26 1181.88 117.316 1177.3 Q120.394 1172.69 126.205 1172.69 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M146.366 1202 L151.251 1202 L151.251 1207.88 L146.366 1207.88 L146.366 1202 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M171.436 1176.4 Q167.825 1176.4 165.996 1179.96 Q164.19 1183.5 164.19 1190.63 Q164.19 1197.74 165.996 1201.3 Q167.825 1204.84 171.436 1204.84 Q175.07 1204.84 176.876 1201.3 Q178.704 1197.74 178.704 1190.63 Q178.704 1183.5 176.876 1179.96 Q175.07 1176.4 171.436 1176.4 M171.436 1172.69 Q177.246 1172.69 180.301 1177.3 Q183.38 1181.88 183.38 1190.63 Q183.38 1199.36 180.301 1203.97 Q177.246 1208.55 171.436 1208.55 Q165.626 1208.55 162.547 1203.97 Q159.491 1199.36 159.491 1190.63 Q159.491 1181.88 162.547 1177.3 Q165.626 1172.69 171.436 1172.69 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M192.408 1203.94 L200.047 1203.94 L200.047 1177.58 L191.737 1179.24 L191.737 1174.98 L200 1173.32 L204.676 1173.32 L204.676 1203.94 L212.315 1203.94 L212.315 1207.88 L192.408 1207.88 L192.408 1203.94 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M231.76 1176.4 Q228.148 1176.4 226.32 1179.96 Q224.514 1183.5 224.514 1190.63 Q224.514 1197.74 226.32 1201.3 Q228.148 1204.84 231.76 1204.84 Q235.394 1204.84 237.199 1201.3 Q239.028 1197.74 239.028 1190.63 Q239.028 1183.5 237.199 1179.96 Q235.394 1176.4 231.76 1176.4 M231.76 1172.69 Q237.57 1172.69 240.625 1177.3 Q243.704 1181.88 243.704 1190.63 Q243.704 1199.36 240.625 1203.97 Q237.57 1208.55 231.76 1208.55 Q225.949 1208.55 222.871 1203.97 Q219.815 1199.36 219.815 1190.63 Q219.815 1181.88 222.871 1177.3 Q225.949 1172.69 231.76 1172.69 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M127.2 953.704 Q123.589 953.704 121.76 957.268 Q119.955 960.81 119.955 967.94 Q119.955 975.046 121.76 978.611 Q123.589 982.153 127.2 982.153 Q130.834 982.153 132.64 978.611 Q134.468 975.046 134.468 967.94 Q134.468 960.81 132.64 957.268 Q130.834 953.704 127.2 953.704 M127.2 950 Q133.01 950 136.066 954.606 Q139.144 959.19 139.144 967.94 Q139.144 976.667 136.066 981.273 Q133.01 985.856 127.2 985.856 Q121.39 985.856 118.311 981.273 Q115.256 976.667 115.256 967.94 Q115.256 959.19 118.311 954.606 Q121.39 950 127.2 950 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M147.362 979.305 L152.246 979.305 L152.246 985.185 L147.362 985.185 L147.362 979.305 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M172.431 953.704 Q168.82 953.704 166.991 957.268 Q165.186 960.81 165.186 967.94 Q165.186 975.046 166.991 978.611 Q168.82 982.153 172.431 982.153 Q176.065 982.153 177.871 978.611 Q179.7 975.046 179.7 967.94 Q179.7 960.81 177.871 957.268 Q176.065 953.704 172.431 953.704 M172.431 950 Q178.241 950 181.297 954.606 Q184.376 959.19 184.376 967.94 Q184.376 976.667 181.297 981.273 Q178.241 985.856 172.431 985.856 Q166.621 985.856 163.542 981.273 Q160.487 976.667 160.487 967.94 Q160.487 959.19 163.542 954.606 Q166.621 950 172.431 950 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M193.403 981.25 L201.042 981.25 L201.042 954.884 L192.732 956.551 L192.732 952.292 L200.996 950.625 L205.672 950.625 L205.672 981.25 L213.311 981.25 L213.311 985.185 L193.403 985.185 L193.403 981.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M222.801 950.625 L241.158 950.625 L241.158 954.56 L227.084 954.56 L227.084 963.032 Q228.102 962.685 229.121 962.523 Q230.139 962.338 231.158 962.338 Q236.945 962.338 240.324 965.509 Q243.704 968.68 243.704 974.097 Q243.704 979.676 240.232 982.778 Q236.76 985.856 230.44 985.856 Q228.264 985.856 225.996 985.486 Q223.75 985.116 221.343 984.375 L221.343 979.676 Q223.426 980.81 225.648 981.366 Q227.871 981.921 230.347 981.921 Q234.352 981.921 236.69 979.815 Q239.028 977.708 239.028 974.097 Q239.028 970.486 236.69 968.38 Q234.352 966.273 230.347 966.273 Q228.473 966.273 226.598 966.69 Q224.746 967.106 222.801 967.986 L222.801 950.625 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M126.205 731.012 Q122.593 731.012 120.765 734.576 Q118.959 738.118 118.959 745.248 Q118.959 752.354 120.765 755.919 Q122.593 759.46 126.205 759.46 Q129.839 759.46 131.644 755.919 Q133.473 752.354 133.473 745.248 Q133.473 738.118 131.644 734.576 Q129.839 731.012 126.205 731.012 M126.205 727.308 Q132.015 727.308 135.07 731.914 Q138.149 736.498 138.149 745.248 Q138.149 753.974 135.07 758.581 Q132.015 763.164 126.205 763.164 Q120.394 763.164 117.316 758.581 Q114.26 753.974 114.26 745.248 Q114.26 736.498 117.316 731.914 Q120.394 727.308 126.205 727.308 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M146.366 756.613 L151.251 756.613 L151.251 762.493 L146.366 762.493 L146.366 756.613 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M171.436 731.012 Q167.825 731.012 165.996 734.576 Q164.19 738.118 164.19 745.248 Q164.19 752.354 165.996 755.919 Q167.825 759.46 171.436 759.46 Q175.07 759.46 176.876 755.919 Q178.704 752.354 178.704 745.248 Q178.704 738.118 176.876 734.576 Q175.07 731.012 171.436 731.012 M171.436 727.308 Q177.246 727.308 180.301 731.914 Q183.38 736.498 183.38 745.248 Q183.38 753.974 180.301 758.581 Q177.246 763.164 171.436 763.164 Q165.626 763.164 162.547 758.581 Q159.491 753.974 159.491 745.248 Q159.491 736.498 162.547 731.914 Q165.626 727.308 171.436 727.308 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M195.625 758.558 L211.945 758.558 L211.945 762.493 L190 762.493 L190 758.558 Q192.663 755.803 197.246 751.173 Q201.852 746.521 203.033 745.178 Q205.278 742.655 206.158 740.919 Q207.061 739.16 207.061 737.47 Q207.061 734.715 205.116 732.979 Q203.195 731.243 200.093 731.243 Q197.894 731.243 195.44 732.007 Q193.01 732.771 190.232 734.322 L190.232 729.599 Q193.056 728.465 195.51 727.887 Q197.963 727.308 200 727.308 Q205.371 727.308 208.565 729.993 Q211.76 732.678 211.76 737.169 Q211.76 739.299 210.949 741.22 Q210.162 743.118 208.056 745.711 Q207.477 746.382 204.375 749.599 Q201.274 752.794 195.625 758.558 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M231.76 731.012 Q228.148 731.012 226.32 734.576 Q224.514 738.118 224.514 745.248 Q224.514 752.354 226.32 755.919 Q228.148 759.46 231.76 759.46 Q235.394 759.46 237.199 755.919 Q239.028 752.354 239.028 745.248 Q239.028 738.118 237.199 734.576 Q235.394 731.012 231.76 731.012 M231.76 727.308 Q237.57 727.308 240.625 731.914 Q243.704 736.498 243.704 745.248 Q243.704 753.974 240.625 758.581 Q237.57 763.164 231.76 763.164 Q225.949 763.164 222.871 758.581 Q219.815 753.974 219.815 745.248 Q219.815 736.498 222.871 731.914 Q225.949 727.308 231.76 727.308 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M127.2 508.319 Q123.589 508.319 121.76 511.884 Q119.955 515.426 119.955 522.555 Q119.955 529.662 121.76 533.227 Q123.589 536.768 127.2 536.768 Q130.834 536.768 132.64 533.227 Q134.468 529.662 134.468 522.555 Q134.468 515.426 132.64 511.884 Q130.834 508.319 127.2 508.319 M127.2 504.616 Q133.01 504.616 136.066 509.222 Q139.144 513.805 139.144 522.555 Q139.144 531.282 136.066 535.889 Q133.01 540.472 127.2 540.472 Q121.39 540.472 118.311 535.889 Q115.256 531.282 115.256 522.555 Q115.256 513.805 118.311 509.222 Q121.39 504.616 127.2 504.616 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M147.362 533.921 L152.246 533.921 L152.246 539.801 L147.362 539.801 L147.362 533.921 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M172.431 508.319 Q168.82 508.319 166.991 511.884 Q165.186 515.426 165.186 522.555 Q165.186 529.662 166.991 533.227 Q168.82 536.768 172.431 536.768 Q176.065 536.768 177.871 533.227 Q179.7 529.662 179.7 522.555 Q179.7 515.426 177.871 511.884 Q176.065 508.319 172.431 508.319 M172.431 504.616 Q178.241 504.616 181.297 509.222 Q184.376 513.805 184.376 522.555 Q184.376 531.282 181.297 535.889 Q178.241 540.472 172.431 540.472 Q166.621 540.472 163.542 535.889 Q160.487 531.282 160.487 522.555 Q160.487 513.805 163.542 509.222 Q166.621 504.616 172.431 504.616 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M196.621 535.866 L212.94 535.866 L212.94 539.801 L190.996 539.801 L190.996 535.866 Q193.658 533.111 198.241 528.481 Q202.848 523.829 204.028 522.486 Q206.274 519.963 207.153 518.227 Q208.056 516.467 208.056 514.778 Q208.056 512.023 206.112 510.287 Q204.19 508.551 201.088 508.551 Q198.889 508.551 196.436 509.315 Q194.005 510.079 191.227 511.63 L191.227 506.907 Q194.051 505.773 196.505 505.194 Q198.959 504.616 200.996 504.616 Q206.366 504.616 209.561 507.301 Q212.755 509.986 212.755 514.477 Q212.755 516.606 211.945 518.528 Q211.158 520.426 209.051 523.018 Q208.473 523.69 205.371 526.907 Q202.269 530.102 196.621 535.866 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M222.801 505.241 L241.158 505.241 L241.158 509.176 L227.084 509.176 L227.084 517.648 Q228.102 517.301 229.121 517.139 Q230.139 516.954 231.158 516.954 Q236.945 516.954 240.324 520.125 Q243.704 523.296 243.704 528.713 Q243.704 534.291 240.232 537.393 Q236.76 540.472 230.44 540.472 Q228.264 540.472 225.996 540.102 Q223.75 539.731 221.343 538.99 L221.343 534.291 Q223.426 535.426 225.648 535.981 Q227.871 536.537 230.347 536.537 Q234.352 536.537 236.69 534.43 Q239.028 532.324 239.028 528.713 Q239.028 525.102 236.69 522.995 Q234.352 520.889 230.347 520.889 Q228.473 520.889 226.598 521.305 Q224.746 521.722 222.801 522.602 L222.801 505.241 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M126.205 285.627 Q122.593 285.627 120.765 289.192 Q118.959 292.734 118.959 299.863 Q118.959 306.97 120.765 310.534 Q122.593 314.076 126.205 314.076 Q129.839 314.076 131.644 310.534 Q133.473 306.97 133.473 299.863 Q133.473 292.734 131.644 289.192 Q129.839 285.627 126.205 285.627 M126.205 281.924 Q132.015 281.924 135.07 286.53 Q138.149 291.113 138.149 299.863 Q138.149 308.59 135.07 313.197 Q132.015 317.78 126.205 317.78 Q120.394 317.78 117.316 313.197 Q114.26 308.59 114.26 299.863 Q114.26 291.113 117.316 286.53 Q120.394 281.924 126.205 281.924 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M146.366 311.229 L151.251 311.229 L151.251 317.109 L146.366 317.109 L146.366 311.229 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M171.436 285.627 Q167.825 285.627 165.996 289.192 Q164.19 292.734 164.19 299.863 Q164.19 306.97 165.996 310.534 Q167.825 314.076 171.436 314.076 Q175.07 314.076 176.876 310.534 Q178.704 306.97 178.704 299.863 Q178.704 292.734 176.876 289.192 Q175.07 285.627 171.436 285.627 M171.436 281.924 Q177.246 281.924 180.301 286.53 Q183.38 291.113 183.38 299.863 Q183.38 308.59 180.301 313.197 Q177.246 317.78 171.436 317.78 Q165.626 317.78 162.547 313.197 Q159.491 308.59 159.491 299.863 Q159.491 291.113 162.547 286.53 Q165.626 281.924 171.436 281.924 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M205.764 298.474 Q209.121 299.192 210.996 301.46 Q212.894 303.729 212.894 307.062 Q212.894 312.178 209.375 314.979 Q205.857 317.78 199.375 317.78 Q197.2 317.78 194.885 317.34 Q192.593 316.923 190.139 316.067 L190.139 311.553 Q192.084 312.687 194.399 313.266 Q196.713 313.845 199.237 313.845 Q203.635 313.845 205.926 312.109 Q208.241 310.372 208.241 307.062 Q208.241 304.007 206.088 302.294 Q203.959 300.558 200.139 300.558 L196.112 300.558 L196.112 296.715 L200.325 296.715 Q203.774 296.715 205.602 295.349 Q207.431 293.96 207.431 291.368 Q207.431 288.706 205.533 287.294 Q203.658 285.859 200.139 285.859 Q198.218 285.859 196.019 286.275 Q193.82 286.692 191.181 287.572 L191.181 283.405 Q193.843 282.664 196.158 282.294 Q198.496 281.924 200.556 281.924 Q205.88 281.924 208.982 284.354 Q212.084 286.761 212.084 290.882 Q212.084 293.752 210.44 295.743 Q208.797 297.71 205.764 298.474 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M231.76 285.627 Q228.148 285.627 226.32 289.192 Q224.514 292.734 224.514 299.863 Q224.514 306.97 226.32 310.534 Q228.148 314.076 231.76 314.076 Q235.394 314.076 237.199 310.534 Q239.028 306.97 239.028 299.863 Q239.028 292.734 237.199 289.192 Q235.394 285.627 231.76 285.627 M231.76 281.924 Q237.57 281.924 240.625 286.53 Q243.704 291.113 243.704 299.863 Q243.704 308.59 240.625 313.197 Q237.57 317.78 231.76 317.78 Q225.949 317.78 222.871 313.197 Q219.815 308.59 219.815 299.863 Q219.815 291.113 222.871 286.53 Q225.949 281.924 231.76 281.924 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M127.2 62.935 Q123.589 62.935 121.76 66.4998 Q119.955 70.0415 119.955 77.1711 Q119.955 84.2775 121.76 87.8423 Q123.589 91.384 127.2 91.384 Q130.834 91.384 132.64 87.8423 Q134.468 84.2775 134.468 77.1711 Q134.468 70.0415 132.64 66.4998 Q130.834 62.935 127.2 62.935 M127.2 59.2314 Q133.01 59.2314 136.066 63.8378 Q139.144 68.4211 139.144 77.1711 Q139.144 85.8979 136.066 90.5043 Q133.01 95.0876 127.2 95.0876 Q121.39 95.0876 118.311 90.5043 Q115.256 85.8979 115.256 77.1711 Q115.256 68.4211 118.311 63.8378 Q121.39 59.2314 127.2 59.2314 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M147.362 88.5368 L152.246 88.5368 L152.246 94.4164 L147.362 94.4164 L147.362 88.5368 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M172.431 62.935 Q168.82 62.935 166.991 66.4998 Q165.186 70.0415 165.186 77.1711 Q165.186 84.2775 166.991 87.8423 Q168.82 91.384 172.431 91.384 Q176.065 91.384 177.871 87.8423 Q179.7 84.2775 179.7 77.1711 Q179.7 70.0415 177.871 66.4998 Q176.065 62.935 172.431 62.935 M172.431 59.2314 Q178.241 59.2314 181.297 63.8378 Q184.376 68.4211 184.376 77.1711 Q184.376 85.8979 181.297 90.5043 Q178.241 95.0876 172.431 95.0876 Q166.621 95.0876 163.542 90.5043 Q160.487 85.8979 160.487 77.1711 Q160.487 68.4211 163.542 63.8378 Q166.621 59.2314 172.431 59.2314 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M206.76 75.7822 Q210.116 76.4998 211.991 78.7683 Q213.889 81.0368 213.889 84.3701 Q213.889 89.4858 210.371 92.2867 Q206.852 95.0876 200.371 95.0876 Q198.195 95.0876 195.88 94.6478 Q193.588 94.2312 191.135 93.3747 L191.135 88.8608 Q193.079 89.9951 195.394 90.5738 Q197.709 91.1525 200.232 91.1525 Q204.63 91.1525 206.922 89.4164 Q209.237 87.6803 209.237 84.3701 Q209.237 81.3146 207.084 79.6016 Q204.954 77.8655 201.135 77.8655 L197.107 77.8655 L197.107 74.0229 L201.32 74.0229 Q204.769 74.0229 206.598 72.6572 Q208.426 71.2683 208.426 68.6758 Q208.426 66.0137 206.528 64.6017 Q204.653 63.1665 201.135 63.1665 Q199.213 63.1665 197.014 63.5832 Q194.815 63.9999 192.176 64.8795 L192.176 60.7128 Q194.838 59.9721 197.153 59.6017 Q199.491 59.2314 201.551 59.2314 Q206.875 59.2314 209.977 61.6619 Q213.079 64.0693 213.079 68.1896 Q213.079 71.06 211.436 73.0507 Q209.792 75.0183 206.76 75.7822 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M222.801 59.8564 L241.158 59.8564 L241.158 63.7915 L227.084 63.7915 L227.084 72.2637 Q228.102 71.9165 229.121 71.7544 Q230.139 71.5693 231.158 71.5693 Q236.945 71.5693 240.324 74.7405 Q243.704 77.9118 243.704 83.3285 Q243.704 88.9071 240.232 92.009 Q236.76 95.0876 230.44 95.0876 Q228.264 95.0876 225.996 94.7173 Q223.75 94.3469 221.343 93.6062 L221.343 88.9071 Q223.426 90.0414 225.648 90.5969 Q227.871 91.1525 230.347 91.1525 Q234.352 91.1525 236.69 89.046 Q239.028 86.9395 239.028 83.3285 Q239.028 79.7174 236.69 77.6109 Q234.352 75.5044 230.347 75.5044 Q228.473 75.5044 226.598 75.9211 Q224.746 76.3377 222.801 77.2174 L222.801 59.8564 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M20.1444 869.878 L26.9239 869.878 Q23.9002 873.125 22.4043 876.817 Q20.9083 880.477 20.9083 884.615 Q20.9083 892.763 25.9054 897.092 Q30.8707 901.421 40.2919 901.421 Q49.6813 901.421 54.6784 897.092 Q59.6436 892.763 59.6436 884.615 Q59.6436 880.477 58.1477 876.817 Q56.6518 873.125 53.6281 869.878 L60.3439 869.878 Q62.6355 873.252 63.7814 877.04 Q64.9272 880.796 64.9272 884.997 Q64.9272 895.787 58.3387 901.993 Q51.7183 908.2 40.2919 908.2 Q28.8336 908.2 22.2451 901.993 Q15.6248 895.787 15.6248 884.997 Q15.6248 880.732 16.7706 876.976 Q17.8846 873.189 20.1444 869.878 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M32.4621 846.389 Q32.4621 851.1 36.1542 853.837 Q39.8145 856.574 46.212 856.574 Q52.6095 856.574 56.3017 853.869 Q59.9619 851.131 59.9619 846.389 Q59.9619 841.71 56.2698 838.973 Q52.5777 836.236 46.212 836.236 Q39.8781 836.236 36.186 838.973 Q32.4621 841.71 32.4621 846.389 M27.4968 846.389 Q27.4968 838.75 32.4621 834.39 Q37.4273 830.029 46.212 830.029 Q54.9649 830.029 59.9619 834.39 Q64.9272 838.75 64.9272 846.389 Q64.9272 854.06 59.9619 858.42 Q54.9649 862.749 46.212 862.749 Q37.4273 862.749 32.4621 858.42 Q27.4968 854.06 27.4968 846.389 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M29.4065 797.596 L34.9447 797.596 Q33.6716 800.079 33.035 802.752 Q32.3984 805.426 32.3984 808.29 Q32.3984 812.651 33.7352 814.847 Q35.072 817.011 37.7456 817.011 Q39.7826 817.011 40.9603 815.452 Q42.1061 813.892 43.1565 809.181 L43.6021 807.176 Q44.9389 800.938 47.3897 798.328 Q49.8086 795.686 54.1691 795.686 Q59.1344 795.686 62.0308 799.633 Q64.9272 803.548 64.9272 810.423 Q64.9272 813.287 64.3543 816.407 Q63.8132 819.494 62.6992 822.931 L56.6518 822.931 Q58.3387 819.685 59.198 816.534 Q60.0256 813.383 60.0256 810.295 Q60.0256 806.158 58.6251 803.93 Q57.1929 801.702 54.6147 801.702 Q52.2276 801.702 50.9545 803.325 Q49.6813 804.916 48.5037 810.359 L48.0262 812.396 Q46.8804 817.839 44.5251 820.258 Q42.138 822.677 38.0002 822.677 Q32.9713 822.677 30.2341 819.112 Q27.4968 815.547 27.4968 808.991 Q27.4968 805.744 27.9743 802.879 Q28.4517 800.015 29.4065 797.596 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M28.3562 786.36 L28.3562 780.504 L64.0042 780.504 L64.0042 786.36 L28.3562 786.36 M14.479 786.36 L14.479 780.504 L21.895 780.504 L21.895 786.36 L14.479 786.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M42.4881 738.618 L64.0042 738.618 L64.0042 744.474 L42.679 744.474 Q37.6183 744.474 35.1038 746.447 Q32.5894 748.421 32.5894 752.368 Q32.5894 757.11 35.6131 759.847 Q38.6368 762.585 43.8567 762.585 L64.0042 762.585 L64.0042 768.473 L28.3562 768.473 L28.3562 762.585 L33.8944 762.585 Q30.6797 760.484 29.0883 757.651 Q27.4968 754.787 27.4968 751.063 Q27.4968 744.92 31.3163 741.769 Q35.1038 738.618 42.4881 738.618 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M44.7161 696.445 L47.5806 696.445 L47.5806 723.372 Q53.6281 722.99 56.8109 719.743 Q59.9619 716.465 59.9619 710.64 Q59.9619 707.267 59.1344 704.115 Q58.3069 700.933 56.6518 697.813 L62.1899 697.813 Q63.5267 700.964 64.227 704.275 Q64.9272 707.585 64.9272 710.99 Q64.9272 719.521 59.9619 724.518 Q54.9967 729.483 46.5303 729.483 Q37.7774 729.483 32.6531 724.772 Q27.4968 720.03 27.4968 712.009 Q27.4968 704.816 32.1438 700.646 Q36.7589 696.445 44.7161 696.445 M42.9973 702.301 Q38.1912 702.365 35.3266 705.007 Q32.4621 707.617 32.4621 711.945 Q32.4621 716.847 35.2312 719.807 Q38.0002 722.735 43.0292 723.181 L42.9973 702.301 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M29.4065 643.387 L34.9447 643.387 Q33.6716 645.869 33.035 648.543 Q32.3984 651.216 32.3984 654.081 Q32.3984 658.442 33.7352 660.638 Q35.072 662.802 37.7456 662.802 Q39.7826 662.802 40.9603 661.242 Q42.1061 659.683 43.1565 654.972 L43.6021 652.967 Q44.9389 646.729 47.3897 644.119 Q49.8086 641.477 54.1691 641.477 Q59.1344 641.477 62.0308 645.424 Q64.9272 649.339 64.9272 656.214 Q64.9272 659.078 64.3543 662.197 Q63.8132 665.285 62.6992 668.722 L56.6518 668.722 Q58.3387 665.476 59.198 662.325 Q60.0256 659.174 60.0256 656.086 Q60.0256 651.949 58.6251 649.721 Q57.1929 647.493 54.6147 647.493 Q52.2276 647.493 50.9545 649.116 Q49.6813 650.707 48.5037 656.15 L48.0262 658.187 Q46.8804 663.63 44.5251 666.049 Q42.138 668.468 38.0002 668.468 Q32.9713 668.468 30.2341 664.903 Q27.4968 661.338 27.4968 654.781 Q27.4968 651.535 27.9743 648.67 Q28.4517 645.806 29.4065 643.387 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M28.3562 632.151 L28.3562 626.295 L64.0042 626.295 L64.0042 632.151 L28.3562 632.151 M14.479 632.151 L14.479 626.295 L21.895 626.295 L21.895 632.151 L14.479 632.151 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M35.1993 586.286 Q31.2526 584.09 29.3747 581.035 Q27.4968 577.979 27.4968 573.841 Q27.4968 568.271 31.4117 565.248 Q35.2948 562.224 42.4881 562.224 L64.0042 562.224 L64.0042 568.112 L42.679 568.112 Q37.5546 568.112 35.072 569.926 Q32.5894 571.741 32.5894 575.465 Q32.5894 580.016 35.6131 582.658 Q38.6368 585.3 43.8567 585.3 L64.0042 585.3 L64.0042 591.188 L42.679 591.188 Q37.5228 591.188 35.072 593.002 Q32.5894 594.816 32.5894 598.604 Q32.5894 603.092 35.6449 605.733 Q38.6686 608.375 43.8567 608.375 L64.0042 608.375 L64.0042 614.264 L28.3562 614.264 L28.3562 608.375 L33.8944 608.375 Q30.616 606.37 29.0564 603.569 Q27.4968 600.768 27.4968 596.917 Q27.4968 593.034 29.4702 590.328 Q31.4436 587.591 35.1993 586.286 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip272)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.7; fill:none\" points=\"\n",
       "  338.375,946.514 344.851,926.458 351.327,907.717 357.803,892.12 364.279,778.708 370.754,717.309 377.23,630.898 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip272)\" style=\"stroke:#e26f46; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.7; fill:none\" points=\"\n",
       "  338.375,1384.24 344.851,1251.4 351.327,1248.8 357.803,1238.09 364.279,1228.18 370.754,1207.18 377.23,1201.92 383.706,1198.98 390.182,1180.9 396.658,1179.27 \n",
       "  403.134,1178.96 409.61,1176.49 416.085,1170.59 422.561,1168.82 429.037,1159.33 435.513,1156.92 441.989,1153.5 448.465,1144.46 454.941,1144.31 461.417,1139.5 \n",
       "  467.892,1137.07 474.368,1126.67 480.844,1114.92 487.32,1113.92 493.796,1111.73 500.272,1108.21 506.748,1105.4 513.223,1099.79 519.699,1096.15 526.175,1095.59 \n",
       "  532.651,1091.42 539.127,1087.47 545.603,1086.5 552.079,1074.71 558.554,1071.2 565.03,1066.22 571.506,1063.5 577.982,1061.02 584.458,1060.69 590.934,1059.81 \n",
       "  597.41,1058.92 603.885,1054.91 610.361,1051.84 616.837,1051.19 623.313,1051.07 629.789,1051.02 636.265,1050.4 642.741,1047.4 649.216,1047.4 655.692,1046.49 \n",
       "  662.168,1042.32 668.644,1038 675.12,1034.81 681.596,1031.2 688.072,1027.79 694.547,1025.34 701.023,1024.6 707.499,1022.47 713.975,1018.05 720.451,1015.88 \n",
       "  726.927,1015.29 733.403,1011.97 739.878,1011.86 746.354,1008.69 752.83,1007.16 759.306,1006.95 765.782,1005.47 772.258,1004.99 778.734,1004.16 785.209,1002.77 \n",
       "  791.685,1002.55 798.161,1001.4 804.637,1000.48 811.113,999.379 817.589,997.811 824.065,997.556 830.54,997.324 837.016,995.707 843.492,994.893 849.968,992.22 \n",
       "  856.444,990.735 862.92,988.106 869.396,987.861 875.871,987.849 882.347,986.038 888.823,983.956 895.299,982.869 901.775,982.814 908.251,980.856 914.727,979.972 \n",
       "  921.203,978.934 927.678,977.817 934.154,972.756 940.63,971.872 947.106,970.236 953.582,969.809 960.058,968.197 966.534,965.377 973.009,965.106 979.485,962.835 \n",
       "  985.961,962.389 992.437,959.098 998.913,958.6 1005.39,957.629 1011.86,957.356 1018.34,956.388 1024.82,953.341 1031.29,952.786 1037.77,951.664 1044.24,949.643 \n",
       "  1050.72,945.862 1057.2,942.811 1063.67,942.56 1070.15,941.912 1076.62,941.065 1083.1,940.888 1089.57,940.604 1096.05,940.367 1102.53,939.06 1109,938.36 \n",
       "  1115.48,935.822 1121.95,934.11 1128.43,932.819 1134.91,931.579 1141.38,929.577 1147.86,928.058 1154.33,926.46 1160.81,925.201 1167.29,924.925 1173.76,923.255 \n",
       "  1180.24,922.554 1186.71,919.985 1193.19,916.587 1199.66,915.53 1206.14,914.967 1212.62,914.757 1219.09,912.727 1225.57,912.366 1232.04,911.745 1238.52,910.618 \n",
       "  1245,910.199 1251.47,908.92 1257.95,906.959 1264.42,906.959 1270.9,906.832 1277.37,902.28 1283.85,902.207 1290.33,901.482 1296.8,900.73 1303.28,900.721 \n",
       "  1309.75,899.564 1316.23,898.386 1322.71,898.131 1329.18,895.91 1335.66,895.35 1342.13,894.931 1348.61,893.791 1355.09,892.735 1361.56,892.262 1368.04,891.977 \n",
       "  1374.51,890.649 1380.99,890.639 1387.46,890.552 1393.94,889.903 1400.42,889.852 1406.89,886.611 1413.37,886.472 1419.84,886.378 1426.32,883.628 1432.8,883.587 \n",
       "  1439.27,882.73 1445.75,881.884 1452.22,881.82 1458.7,878.633 1465.17,876.339 1471.65,874.415 1478.13,872.244 1484.6,871.877 1491.08,870.723 1497.55,868.618 \n",
       "  1504.03,868.111 1510.51,866.972 1516.98,866.444 1523.46,865.738 1529.93,865.027 1536.41,864.883 1542.88,863.74 1549.36,862.568 1555.84,862.044 1562.31,861.323 \n",
       "  1568.79,856.111 1575.26,850.939 1581.74,849.437 1588.22,846.997 1594.69,845.921 1601.17,843.432 1607.64,838.899 1614.12,837.746 1620.6,834.792 1627.07,831.577 \n",
       "  1633.55,830.726 1640.02,824.588 1646.5,818.572 1652.97,817.025 1659.45,814.414 1665.93,811.483 1672.4,811.255 1678.88,809.026 1685.35,804.055 1691.83,799.825 \n",
       "  1698.31,797.191 1704.78,794.627 1711.26,794.339 1717.73,788.673 1724.21,786.204 1730.68,785.263 1737.16,784.554 1743.64,783.901 1750.11,783.755 1756.59,778.376 \n",
       "  1763.06,774.06 1769.54,771.542 1776.02,769.988 1782.49,765.74 1788.97,763.293 1795.44,762.188 1801.92,760.127 1808.4,742.185 1814.87,734.345 1821.35,726.226 \n",
       "  1827.82,720.359 1834.3,719.436 1840.77,717.402 1847.25,713.476 1853.73,712.944 1860.2,711.352 1866.68,708.116 1873.15,705.584 1879.63,705.048 1886.11,702.167 \n",
       "  1892.58,701.778 1899.06,696.147 1905.53,691.345 1912.01,683.026 1918.48,682.291 1924.96,682.027 1931.44,679.154 1937.91,670.332 1944.39,668.418 1950.86,667.526 \n",
       "  1957.34,664.911 1963.82,654.288 1970.29,651.534 1976.77,640.809 1983.24,630.024 1989.72,629.102 1996.2,622.489 2002.67,617.611 2009.15,616.109 2015.62,604.067 \n",
       "  2022.1,599.589 2028.57,597.589 2035.05,594.564 2041.53,591.463 2048,587.614 2054.48,585.014 2060.95,575.814 2067.43,565.316 2073.91,559.562 2080.38,557.925 \n",
       "  2086.86,553.41 2093.33,548.178 2099.81,538.831 2106.28,516.035 2112.76,505.085 2119.24,481.033 2125.71,476.634 2132.19,461.272 2138.66,452.301 2145.14,444.946 \n",
       "  2151.62,443.816 2158.09,437.624 2164.57,435.339 2171.04,428.435 2177.52,421.946 2184,407.248 2190.47,405.236 2196.95,400.828 2203.42,394.681 2209.9,391.126 \n",
       "  2216.37,381.998 2222.85,370.134 2229.33,367.411 2235.8,364.19 2242.28,362.449 2248.75,357.262 2255.23,343.149 2261.71,327.46 2268.18,255.872 2274.66,240.268 \n",
       "  2281.13,217.04 2287.61,169.807 2294.08,86.1857 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip270)\" d=\"\n",
       "M1745.31 1377.32 L2283.65 1377.32 L2283.65 1221.8 L1745.31 1221.8  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip270)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1745.31,1377.32 2283.65,1377.32 2283.65,1221.8 1745.31,1221.8 1745.31,1377.32 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip270)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.7; fill:none\" points=\"\n",
       "  1768.34,1273.64 1906.54,1273.64 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip270)\" d=\"M1933.86 1287.03 L1933.86 1300.78 L1929.58 1300.78 L1929.58 1264.99 L1933.86 1264.99 L1933.86 1268.92 Q1935.2 1266.61 1937.24 1265.5 Q1939.3 1264.36 1942.15 1264.36 Q1946.87 1264.36 1949.81 1268.11 Q1952.77 1271.86 1952.77 1277.98 Q1952.77 1284.09 1949.81 1287.84 Q1946.87 1291.59 1942.15 1291.59 Q1939.3 1291.59 1937.24 1290.48 Q1935.2 1289.34 1933.86 1287.03 M1948.35 1277.98 Q1948.35 1273.28 1946.41 1270.61 Q1944.49 1267.93 1941.11 1267.93 Q1937.73 1267.93 1935.78 1270.61 Q1933.86 1273.28 1933.86 1277.98 Q1933.86 1282.67 1935.78 1285.36 Q1937.73 1288.02 1941.11 1288.02 Q1944.49 1288.02 1946.41 1285.36 Q1948.35 1282.67 1948.35 1277.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M1969.88 1267.98 Q1966.45 1267.98 1964.46 1270.66 Q1962.47 1273.32 1962.47 1277.98 Q1962.47 1282.63 1964.44 1285.31 Q1966.43 1287.98 1969.88 1287.98 Q1973.28 1287.98 1975.27 1285.29 Q1977.26 1282.61 1977.26 1277.98 Q1977.26 1273.37 1975.27 1270.68 Q1973.28 1267.98 1969.88 1267.98 M1969.88 1264.36 Q1975.43 1264.36 1978.61 1267.98 Q1981.78 1271.59 1981.78 1277.98 Q1981.78 1284.34 1978.61 1287.98 Q1975.43 1291.59 1969.88 1291.59 Q1964.3 1291.59 1961.13 1287.98 Q1957.98 1284.34 1957.98 1277.98 Q1957.98 1271.59 1961.13 1267.98 Q1964.3 1264.36 1969.88 1264.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M2005.37 1265.75 L2005.37 1269.78 Q2003.56 1268.86 2001.62 1268.39 Q1999.67 1267.93 1997.59 1267.93 Q1994.42 1267.93 1992.82 1268.9 Q1991.25 1269.87 1991.25 1271.82 Q1991.25 1273.3 1992.38 1274.16 Q1993.51 1274.99 1996.94 1275.75 L1998.4 1276.08 Q2002.93 1277.05 2004.83 1278.83 Q2006.75 1280.59 2006.75 1283.76 Q2006.75 1287.37 2003.88 1289.48 Q2001.04 1291.59 1996.04 1291.59 Q1993.95 1291.59 1991.68 1291.17 Q1989.44 1290.78 1986.94 1289.97 L1986.94 1285.57 Q1989.3 1286.79 1991.59 1287.42 Q1993.88 1288.02 1996.13 1288.02 Q1999.14 1288.02 2000.76 1287 Q2002.38 1285.96 2002.38 1284.09 Q2002.38 1282.35 2001.2 1281.42 Q2000.04 1280.5 1996.08 1279.64 L1994.6 1279.3 Q1990.64 1278.46 1988.88 1276.75 Q1987.12 1275.01 1987.12 1272 Q1987.12 1268.35 1989.72 1266.36 Q1992.31 1264.36 1997.08 1264.36 Q1999.44 1264.36 2001.52 1264.71 Q2003.61 1265.06 2005.37 1265.75 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M2013.54 1264.99 L2017.8 1264.99 L2017.8 1290.92 L2013.54 1290.92 L2013.54 1264.99 M2013.54 1254.9 L2017.8 1254.9 L2017.8 1260.29 L2013.54 1260.29 L2013.54 1254.9 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M2030.92 1257.63 L2030.92 1264.99 L2039.69 1264.99 L2039.69 1268.3 L2030.92 1268.3 L2030.92 1282.37 Q2030.92 1285.54 2031.78 1286.45 Q2032.66 1287.35 2035.32 1287.35 L2039.69 1287.35 L2039.69 1290.92 L2035.32 1290.92 Q2030.39 1290.92 2028.51 1289.09 Q2026.64 1287.23 2026.64 1282.37 L2026.64 1268.3 L2023.51 1268.3 L2023.51 1264.99 L2026.64 1264.99 L2026.64 1257.63 L2030.92 1257.63 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M2045.3 1264.99 L2049.55 1264.99 L2049.55 1290.92 L2045.3 1290.92 L2045.3 1264.99 M2045.3 1254.9 L2049.55 1254.9 L2049.55 1260.29 L2045.3 1260.29 L2045.3 1254.9 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M2055.41 1264.99 L2059.93 1264.99 L2068.03 1286.75 L2076.13 1264.99 L2080.64 1264.99 L2070.92 1290.92 L2065.13 1290.92 L2055.41 1264.99 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M2108.7 1276.89 L2108.7 1278.97 L2089.11 1278.97 Q2089.39 1283.37 2091.75 1285.68 Q2094.14 1287.98 2098.37 1287.98 Q2100.83 1287.98 2103.12 1287.37 Q2105.43 1286.77 2107.7 1285.57 L2107.7 1289.6 Q2105.41 1290.57 2103 1291.08 Q2100.6 1291.59 2098.12 1291.59 Q2091.92 1291.59 2088.28 1287.98 Q2084.67 1284.36 2084.67 1278.21 Q2084.67 1271.84 2088.1 1268.11 Q2091.55 1264.36 2097.38 1264.36 Q2102.61 1264.36 2105.64 1267.74 Q2108.7 1271.1 2108.7 1276.89 M2104.44 1275.64 Q2104.39 1272.14 2102.47 1270.06 Q2100.57 1267.98 2097.43 1267.98 Q2093.86 1267.98 2091.71 1269.99 Q2089.58 1272 2089.25 1275.66 L2104.44 1275.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M2134.97 1257.63 L2134.97 1264.99 L2143.74 1264.99 L2143.74 1268.3 L2134.97 1268.3 L2134.97 1282.37 Q2134.97 1285.54 2135.83 1286.45 Q2136.71 1287.35 2139.37 1287.35 L2143.74 1287.35 L2143.74 1290.92 L2139.37 1290.92 Q2134.44 1290.92 2132.56 1289.09 Q2130.69 1287.23 2130.69 1282.37 L2130.69 1268.3 L2127.56 1268.3 L2127.56 1264.99 L2130.69 1264.99 L2130.69 1257.63 L2134.97 1257.63 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M2171.52 1276.89 L2171.52 1278.97 L2151.94 1278.97 Q2152.22 1283.37 2154.58 1285.68 Q2156.96 1287.98 2161.2 1287.98 Q2163.65 1287.98 2165.94 1287.37 Q2168.26 1286.77 2170.53 1285.57 L2170.53 1289.6 Q2168.23 1290.57 2165.83 1291.08 Q2163.42 1291.59 2160.94 1291.59 Q2154.74 1291.59 2151.11 1287.98 Q2147.49 1284.36 2147.49 1278.21 Q2147.49 1271.84 2150.92 1268.11 Q2154.37 1264.36 2160.2 1264.36 Q2165.43 1264.36 2168.47 1267.74 Q2171.52 1271.1 2171.52 1276.89 M2167.26 1275.64 Q2167.22 1272.14 2165.3 1270.06 Q2163.4 1267.98 2160.25 1267.98 Q2156.68 1267.98 2154.53 1269.99 Q2152.4 1272 2152.08 1275.66 L2167.26 1275.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M2195.04 1265.75 L2195.04 1269.78 Q2193.23 1268.86 2191.29 1268.39 Q2189.35 1267.93 2187.26 1267.93 Q2184.09 1267.93 2182.49 1268.9 Q2180.92 1269.87 2180.92 1271.82 Q2180.92 1273.3 2182.05 1274.16 Q2183.19 1274.99 2186.61 1275.75 L2188.07 1276.08 Q2192.61 1277.05 2194.51 1278.83 Q2196.43 1280.59 2196.43 1283.76 Q2196.43 1287.37 2193.56 1289.48 Q2190.71 1291.59 2185.71 1291.59 Q2183.63 1291.59 2181.36 1291.17 Q2179.11 1290.78 2176.61 1289.97 L2176.61 1285.57 Q2178.98 1286.79 2181.27 1287.42 Q2183.56 1288.02 2185.8 1288.02 Q2188.81 1288.02 2190.43 1287 Q2192.05 1285.96 2192.05 1284.09 Q2192.05 1282.35 2190.87 1281.42 Q2189.72 1280.5 2185.76 1279.64 L2184.28 1279.3 Q2180.32 1278.46 2178.56 1276.75 Q2176.8 1275.01 2176.8 1272 Q2176.8 1268.35 2179.39 1266.36 Q2181.98 1264.36 2186.75 1264.36 Q2189.11 1264.36 2191.2 1264.71 Q2193.28 1265.06 2195.04 1265.75 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M2207.42 1257.63 L2207.42 1264.99 L2216.2 1264.99 L2216.2 1268.3 L2207.42 1268.3 L2207.42 1282.37 Q2207.42 1285.54 2208.28 1286.45 Q2209.16 1287.35 2211.82 1287.35 L2216.2 1287.35 L2216.2 1290.92 L2211.82 1290.92 Q2206.89 1290.92 2205.02 1289.09 Q2203.14 1287.23 2203.14 1282.37 L2203.14 1268.3 L2200.02 1268.3 L2200.02 1264.99 L2203.14 1264.99 L2203.14 1257.63 L2207.42 1257.63 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip270)\" style=\"stroke:#e26f46; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:0.7; fill:none\" points=\"\n",
       "  1768.34,1325.48 1906.54,1325.48 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip270)\" d=\"M1951.29 1327.11 L1951.29 1342.76 L1947.03 1342.76 L1947.03 1327.25 Q1947.03 1323.57 1945.6 1321.74 Q1944.16 1319.91 1941.29 1319.91 Q1937.84 1319.91 1935.85 1322.11 Q1933.86 1324.31 1933.86 1328.1 L1933.86 1342.76 L1929.58 1342.76 L1929.58 1316.83 L1933.86 1316.83 L1933.86 1320.86 Q1935.39 1318.52 1937.45 1317.36 Q1939.53 1316.2 1942.24 1316.2 Q1946.71 1316.2 1949 1318.98 Q1951.29 1321.74 1951.29 1327.11 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M1981.96 1328.73 L1981.96 1330.81 L1962.38 1330.81 Q1962.66 1335.21 1965.02 1337.52 Q1967.4 1339.82 1971.64 1339.82 Q1974.09 1339.82 1976.38 1339.21 Q1978.7 1338.61 1980.97 1337.41 L1980.97 1341.44 Q1978.68 1342.41 1976.27 1342.92 Q1973.86 1343.43 1971.38 1343.43 Q1965.18 1343.43 1961.55 1339.82 Q1957.94 1336.2 1957.94 1330.05 Q1957.94 1323.68 1961.36 1319.95 Q1964.81 1316.2 1970.64 1316.2 Q1975.87 1316.2 1978.91 1319.58 Q1981.96 1322.94 1981.96 1328.73 M1977.7 1327.48 Q1977.66 1323.98 1975.74 1321.9 Q1973.84 1319.82 1970.69 1319.82 Q1967.12 1319.82 1964.97 1321.83 Q1962.84 1323.84 1962.52 1327.5 L1977.7 1327.48 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M2006.01 1329.49 Q2006.01 1324.86 2004.09 1322.32 Q2002.19 1319.77 1998.75 1319.77 Q1995.32 1319.77 1993.4 1322.32 Q1991.5 1324.86 1991.5 1329.49 Q1991.5 1334.1 1993.4 1336.64 Q1995.32 1339.19 1998.75 1339.19 Q2002.19 1339.19 2004.09 1336.64 Q2006.01 1334.1 2006.01 1329.49 M2010.27 1339.54 Q2010.27 1346.16 2007.33 1349.38 Q2004.39 1352.62 1998.33 1352.62 Q1996.08 1352.62 1994.09 1352.27 Q1992.1 1351.95 1990.23 1351.25 L1990.23 1347.11 Q1992.1 1348.13 1993.93 1348.61 Q1995.76 1349.1 1997.66 1349.1 Q2001.85 1349.1 2003.93 1346.9 Q2006.01 1344.72 2006.01 1340.3 L2006.01 1338.2 Q2004.69 1340.49 2002.63 1341.62 Q2000.57 1342.76 1997.7 1342.76 Q1992.93 1342.76 1990.02 1339.12 Q1987.1 1335.49 1987.1 1329.49 Q1987.1 1323.47 1990.02 1319.84 Q1992.93 1316.2 1997.7 1316.2 Q2000.57 1316.2 2002.63 1317.34 Q2004.69 1318.47 2006.01 1320.76 L2006.01 1316.83 L2010.27 1316.83 L2010.27 1339.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M2030.83 1329.72 Q2025.67 1329.72 2023.68 1330.9 Q2021.68 1332.08 2021.68 1334.93 Q2021.68 1337.2 2023.17 1338.54 Q2024.67 1339.86 2027.24 1339.86 Q2030.78 1339.86 2032.91 1337.36 Q2035.06 1334.84 2035.06 1330.67 L2035.06 1329.72 L2030.83 1329.72 M2039.32 1327.96 L2039.32 1342.76 L2035.06 1342.76 L2035.06 1338.82 Q2033.61 1341.18 2031.43 1342.32 Q2029.25 1343.43 2026.11 1343.43 Q2022.12 1343.43 2019.76 1341.2 Q2017.43 1338.96 2017.43 1335.21 Q2017.43 1330.83 2020.34 1328.61 Q2023.28 1326.39 2029.09 1326.39 L2035.06 1326.39 L2035.06 1325.97 Q2035.06 1323.03 2033.12 1321.44 Q2031.2 1319.82 2027.7 1319.82 Q2025.48 1319.82 2023.37 1320.35 Q2021.27 1320.88 2019.32 1321.95 L2019.32 1318.01 Q2021.66 1317.11 2023.86 1316.67 Q2026.06 1316.2 2028.14 1316.2 Q2033.77 1316.2 2036.55 1319.12 Q2039.32 1322.04 2039.32 1327.96 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M2052.31 1309.47 L2052.31 1316.83 L2061.08 1316.83 L2061.08 1320.14 L2052.31 1320.14 L2052.31 1334.21 Q2052.31 1337.38 2053.17 1338.29 Q2054.05 1339.19 2056.71 1339.19 L2061.08 1339.19 L2061.08 1342.76 L2056.71 1342.76 Q2051.78 1342.76 2049.9 1340.93 Q2048.03 1339.07 2048.03 1334.21 L2048.03 1320.14 L2044.9 1320.14 L2044.9 1316.83 L2048.03 1316.83 L2048.03 1309.47 L2052.31 1309.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M2066.68 1316.83 L2070.94 1316.83 L2070.94 1342.76 L2066.68 1342.76 L2066.68 1316.83 M2066.68 1306.74 L2070.94 1306.74 L2070.94 1312.13 L2066.68 1312.13 L2066.68 1306.74 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M2076.8 1316.83 L2081.31 1316.83 L2089.42 1338.59 L2097.52 1316.83 L2102.03 1316.83 L2092.31 1342.76 L2086.52 1342.76 L2076.8 1316.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M2130.09 1328.73 L2130.09 1330.81 L2110.5 1330.81 Q2110.78 1335.21 2113.14 1337.52 Q2115.53 1339.82 2119.76 1339.82 Q2122.22 1339.82 2124.51 1339.21 Q2126.82 1338.61 2129.09 1337.41 L2129.09 1341.44 Q2126.8 1342.41 2124.39 1342.92 Q2121.99 1343.43 2119.51 1343.43 Q2113.3 1343.43 2109.67 1339.82 Q2106.06 1336.2 2106.06 1330.05 Q2106.06 1323.68 2109.49 1319.95 Q2112.93 1316.2 2118.77 1316.2 Q2124 1316.2 2127.03 1319.58 Q2130.09 1322.94 2130.09 1328.73 M2125.83 1327.48 Q2125.78 1323.98 2123.86 1321.9 Q2121.96 1319.82 2118.81 1319.82 Q2115.25 1319.82 2113.1 1321.83 Q2110.97 1323.84 2110.64 1327.5 L2125.83 1327.48 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M2156.36 1309.47 L2156.36 1316.83 L2165.13 1316.83 L2165.13 1320.14 L2156.36 1320.14 L2156.36 1334.21 Q2156.36 1337.38 2157.22 1338.29 Q2158.1 1339.19 2160.76 1339.19 L2165.13 1339.19 L2165.13 1342.76 L2160.76 1342.76 Q2155.83 1342.76 2153.95 1340.93 Q2152.08 1339.07 2152.08 1334.21 L2152.08 1320.14 L2148.95 1320.14 L2148.95 1316.83 L2152.08 1316.83 L2152.08 1309.47 L2156.36 1309.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M2192.91 1328.73 L2192.91 1330.81 L2173.33 1330.81 Q2173.61 1335.21 2175.97 1337.52 Q2178.35 1339.82 2182.59 1339.82 Q2185.04 1339.82 2187.33 1339.21 Q2189.65 1338.61 2191.92 1337.41 L2191.92 1341.44 Q2189.62 1342.41 2187.22 1342.92 Q2184.81 1343.43 2182.33 1343.43 Q2176.13 1343.43 2172.49 1339.82 Q2168.88 1336.2 2168.88 1330.05 Q2168.88 1323.68 2172.31 1319.95 Q2175.76 1316.2 2181.59 1316.2 Q2186.82 1316.2 2189.86 1319.58 Q2192.91 1322.94 2192.91 1328.73 M2188.65 1327.48 Q2188.61 1323.98 2186.68 1321.9 Q2184.79 1319.82 2181.64 1319.82 Q2178.07 1319.82 2175.92 1321.83 Q2173.79 1323.84 2173.47 1327.5 L2188.65 1327.48 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M2216.43 1317.59 L2216.43 1321.62 Q2214.62 1320.7 2212.68 1320.23 Q2210.73 1319.77 2208.65 1319.77 Q2205.48 1319.77 2203.88 1320.74 Q2202.31 1321.71 2202.31 1323.66 Q2202.31 1325.14 2203.44 1326 Q2204.58 1326.83 2208 1327.59 L2209.46 1327.92 Q2214 1328.89 2215.9 1330.67 Q2217.82 1332.43 2217.82 1335.6 Q2217.82 1339.21 2214.95 1341.32 Q2212.1 1343.43 2207.1 1343.43 Q2205.02 1343.43 2202.75 1343.01 Q2200.5 1342.62 2198 1341.81 L2198 1337.41 Q2200.36 1338.63 2202.66 1339.26 Q2204.95 1339.86 2207.19 1339.86 Q2210.2 1339.86 2211.82 1338.84 Q2213.44 1337.8 2213.44 1335.93 Q2213.44 1334.19 2212.26 1333.26 Q2211.11 1332.34 2207.15 1331.48 L2205.67 1331.14 Q2201.71 1330.3 2199.95 1328.59 Q2198.19 1326.85 2198.19 1323.84 Q2198.19 1320.19 2200.78 1318.2 Q2203.37 1316.2 2208.14 1316.2 Q2210.5 1316.2 2212.59 1316.55 Q2214.67 1316.9 2216.43 1317.59 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip270)\" d=\"M2228.81 1309.47 L2228.81 1316.83 L2237.59 1316.83 L2237.59 1320.14 L2228.81 1320.14 L2228.81 1334.21 Q2228.81 1337.38 2229.67 1338.29 Q2230.55 1339.19 2233.21 1339.19 L2237.59 1339.19 L2237.59 1342.76 L2233.21 1342.76 Q2228.28 1342.76 2226.41 1340.93 Q2224.53 1339.07 2224.53 1334.21 L2224.53 1320.14 L2221.41 1320.14 L2221.41 1316.83 L2224.53 1316.83 L2224.53 1309.47 L2228.81 1309.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_pos_test = [cos_sim(training_pos_agg, x) for x in testing_pos]\n",
    "dist_neg_test = [cos_sim(training_pos_agg, x) for x in testing_neg]\n",
    "\n",
    "# make plots\n",
    "hist = histogram(dist_pos_test, label=\"positive test\", alpha=0.7, legend=:topleft, nbins=40)\n",
    "histogram!(dist_neg_test, label=\"negative test\", alpha=0.7, nbins=30)\n",
    "#savefig(hist, results_dir*\"/HDC_histogram.png\")\n",
    "\n",
    "rank_plot = plot(sort!(dist_pos_test), label=\"positive test\", alpha=0.7, legend=:bottomright, xlabel=\"rank\", ylabel=\"Cosine sim\")\n",
    "plot!(sort!(dist_neg_test), label=\"negative test\", alpha=0.7)\n",
    "#savefig(rank_plot, results_dir*\"/HDC_rank_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a51883",
   "metadata": {},
   "source": [
    "#### a) HDC model that only considers the positive aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59bba8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the cross-validation and keep the scores\n",
    "#similarities = Dict()\n",
    "similarities = zeros(Float64, length(loci_embeddings), length(rbp_embeddings))\n",
    "#for (train_indices, test_indices) in CrossValidation.StratifiedKFold(labels, n_folds=10, \n",
    "#        shuffle=true, random_state=42)\n",
    "for (test, train_indices) in enumerate(LOOCV(length(signatures)))\n",
    "    # define training and test data\n",
    "    x_train = signatures[train_indices]\n",
    "    y_train = labels[train_indices]\n",
    "    x_test = signatures[test]\n",
    "    y_test = labels[test]\n",
    "    \n",
    "    # train the positive aggregate\n",
    "    pos_aggregate = HyperdimensionalComputing.aggregate(x_train[y_train .== 1])\n",
    "    pos_i, pos_j = positions[test][1], positions[test][2]\n",
    "    similarities[pos_i, pos_j] = cos_sim(pos_aggregate, x_test)\n",
    "    \n",
    "    # compute similarities to the pos aggregate and keep the index\n",
    "    #similarities_k = [cos_sim(pos_aggregate, x) for x in x_test]\n",
    "    #for (i, test_i) in enumerate(test_indices)\n",
    "    #    pos_i, pos_j = positions[test_i][1], positions[test_i][2]\n",
    "    #    similarities[pos_i, pos_j] = similarities_k[i]\n",
    "    #end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "2dbd8e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_cols = [col for col in 1:size(interaction_matrix)[2] if sum(ismissing.(interaction_matrix[:, col])) > 0]\n",
    "missing_sims = [col for col in 1:size(similarities)[2] if sum(similarities[:, col]) == 0]\n",
    "@test missing_cols == missing_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f5e50ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09480120897777944"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrr_from_scores(similarities, interaction_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723d22a3",
   "metadata": {},
   "source": [
    "#### b) HDC model that considers both positive and negative aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a08705b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the cross-validation and keep the scores\n",
    "#similarities = Dict()\n",
    "similarities = zeros(Float64, length(loci_embeddings), length(rbp_embeddings))\n",
    "for (train_indices, test_indices) in CrossValidation.StratifiedKFold(labels, n_folds=10, \n",
    "        shuffle=true, random_state=42)\n",
    "    # define training and test data\n",
    "    x_train = signatures[train_indices]\n",
    "    y_train = labels[train_indices]\n",
    "    x_test = signatures[test_indices]\n",
    "    y_test = labels[test_indices]\n",
    "    \n",
    "    # train the aggregates\n",
    "    centers = train(y_train, x_train)\n",
    "    \n",
    "    # compute ratio of similarities to aggregates (centers[0] -> neg-, centers[1] -> pos+) and keep the index\n",
    "    similarities_k = [cos_sim(centers[1], x)/cos_sim(centers[0], x) for x in x_test]\n",
    "    for (i, test_i) in enumerate(test_indices)\n",
    "        pos_i, pos_j = positions[test_i][1], positions[test_i][2]\n",
    "        similarities[pos_i, pos_j] = similarities_k[i]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c4a388",
   "metadata": {},
   "source": [
    "But this isn't really satisfactory because if the sim scores to pos aggregate are negative, things shift from interpretation (the higher the ratio the closer to pos+ aggregate will not hold anymore... check notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c863bed4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1394852301398791"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrr_from_scores(similarities, interaction_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205933e6",
   "metadata": {},
   "source": [
    "## X - Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "10452d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary: | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "tests for MRR | \u001b[32m   3  \u001b[39m\u001b[36m    3\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "@testset \"tests for MRR\" begin\n",
    "    @test mean_reciprocal_rank([[0, 0, 0], [0, 1, 0], [1, 0, 0]]) == 0.5\n",
    "    @test mean_reciprocal_rank([[1, 0, 0], [1, 1, 0], [1, 0, 0]]) == 1\n",
    "    A = [0 0 0; 0 1 0; 1 0 0] # Matrix\n",
    "    @test mean_reciprocal_rank(A) == 0.5\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "77d726c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:                          | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "tests for constructing relevant matrix | \u001b[32m   3  \u001b[39m\u001b[36m    3\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "@testset \"tests for constructing relevant matrix\" begin\n",
    "    preds = [0 0 1; 1 1 0; 0 0 0]\n",
    "    labels = [0 0 1; 1 1 0; 0 0 0]\n",
    "    @test construct_relevant_matrix(preds, labels) == labels # all TPs\n",
    "    preds = [0 0 0; 1 0 0; 0 0 0]\n",
    "    labels = [0 0 1; 1 1 0; 0 0 0]\n",
    "    @test construct_relevant_matrix(preds, labels) == [0 0 0; 1 0 0; 0 0 0] # missed TPs\n",
    "    preds = [0 1 0; 1 1 0; 1 0 0]\n",
    "    labels = [0 0 1; 1 1 0; 0 0 0]\n",
    "    @test construct_relevant_matrix(preds, labels) == [0 0 0; 1 1 0; 0 0 0] # FPs\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ecf63855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:                  | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "tests for sorting label matrix | \u001b[32m   2  \u001b[39m\u001b[36m    2\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "@testset \"tests for sorting label matrix\" begin \n",
    "    rel = [0 0 1; 1 1 0; 0 0 0]\n",
    "    scores = [0.5 0.3 1; 1 0.3 0.7; 0.2 0.3 0.3]\n",
    "    @test sort_label_matrix(scores, rel) == [1 0 0; 1 0 1; 0 0 0]\n",
    "    \n",
    "    rel = [0 0 1; 1 1 0; 0 0 0]\n",
    "    scores = [1 0.3 1; 1 0.3 0.7; 0.2 0.3 0.3] # equal score for 0 and 1\n",
    "    @test sort_label_matrix(scores, rel) == [0 1 0; 1 0 1; 0 0 0] # equals will appear in ascending order\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "522f6870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:  | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "tests pipeline | \u001b[32m   3  \u001b[39m\u001b[36m    3\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "@testset \"tests pipeline\" begin\n",
    "    preds = [0 0 1; 0 1 0; 0 0 0]\n",
    "    labels = [0 0 1; 1 1 0; 0 0 0]\n",
    "    scores = [0 0.2 0.5; 0.3 0.2 0.1; 0.2 0.4 0.3]\n",
    "    relm = construct_relevant_matrix(preds, labels)\n",
    "    sortm = sort_label_matrix(scores, relm)\n",
    "    @test construct_relevant_matrix(preds, labels) == [0 0 1; 0 1 0; 0 0 0]\n",
    "    @test sort_relevant_matrix(relm, scores) == [1 0 0; 0 1 0; 0 0 0] \n",
    "    @test mean_reciprocal_rank(sortm) == 0.5\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01358f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:         | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "tests mrr from scores | \u001b[32m   1  \u001b[39m\u001b[36m    1\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "@testset \"tests mrr from scores\" begin\n",
    "    labels = [0 0 1; 1 1 0; 0 0 0]\n",
    "    scores = [0 0.2 0.5; 0.3 0.5 0.6; 0.2 0.4 0.3]\n",
    "    @test mrr_from_scores(scores, labels, 0.4) == 0.5\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143556f7",
   "metadata": {},
   "source": [
    "## X - Legacy code"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cee06006",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Here, we perform a 10-fold CV over the loci, just like we do to evaluate the\n",
    "binary classifiers in Python.\n",
    "\"\"\"\n",
    "loci_known = [x for x in range(1, length=length(LociBase)) \n",
    "                if (any(isequal.(interaction_matrix[x,:], 0))) \n",
    "                    || (any(isequal.(interaction_matrix[x,:], 1)))]\n",
    "\n",
    "# shuffle loci\n",
    "loci_shuffle = shuffle(loci_known)\n",
    "\n",
    "# divide into 10 groups\n",
    "group_size = div(length(loci_shuffle), 10) + 1\n",
    "get_groups(x, n) = [x[i:min(i+n-1,length(x))] for i in 1:n:length(x)]\n",
    "loci_groups = get_groups(loci_shuffle, group_size)\n",
    "\n",
    "# loop over groups\n",
    "loci_nr = []; rbp_nr = []; scores = []; scores_pos = []; labels = []\n",
    "for group in loci_groups\n",
    "    # compute signatures for training and testing parts (group = test)\n",
    "    signatures_train_pos = []\n",
    "    signatures_train_neg = []\n",
    "    signatures_test = []\n",
    "    for (i, loci_embedding) in enumerate(loci_embeddings)\n",
    "        for (j, rbp_embedding) in enumerate(rbp_embeddings)\n",
    "            # training pos interaction\n",
    "            if isequal(interaction_matrix[i,j], 1) && i ∉ group\n",
    "                push!(signatures_train_pos, HyperdimensionalComputing.bind([loci_embedding, rbp_embedding]))\n",
    "            # training neg interaction\n",
    "            elseif isequal(interaction_matrix[i,j], 0) && i ∉ group\n",
    "                push!(signatures_train_neg, HyperdimensionalComputing.bind([loci_embedding, rbp_embedding]))\n",
    "            # test interaction\n",
    "            elseif isequal(interaction_matrix[i,j], 1) && i in group\n",
    "                push!(signatures_test, HyperdimensionalComputing.bind([loci_embedding, rbp_embedding]))\n",
    "                push!(loci_nr, i-1) # -1 to cope with indexing python\n",
    "                push!(rbp_nr, j-1)\n",
    "                push!(labels, interaction_matrix[i,j])\n",
    "            elseif isequal(interaction_matrix[i,j], 0) && i in group\n",
    "                push!(signatures_test, HyperdimensionalComputing.bind([loci_embedding, rbp_embedding]))\n",
    "                push!(loci_nr, i-1)\n",
    "                push!(rbp_nr, j-1)\n",
    "                push!(labels, interaction_matrix[i,j])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # convert signatures\n",
    "    signatures_train_pos = convert(Array{BipolarHDV}, signatures_train_pos)\n",
    "    signatures_train_neg = convert(Array{BipolarHDV}, signatures_train_neg)\n",
    "    signatures_test = convert(Array{BipolarHDV}, signatures_test)\n",
    "    println(\"train size:\", length(signatures_train_pos)+length(signatures_train_neg))\n",
    "    println(\"test size:\", length(signatures_test))\n",
    "    \n",
    "    # aggregate training signatures\n",
    "    signatures_pos_agg = HyperdimensionalComputing.aggregate(signatures_train_pos)\n",
    "    signatures_neg_agg = HyperdimensionalComputing.aggregate(signatures_train_neg)\n",
    "\n",
    "    # compute distance/similarity to test signatures\n",
    "    for test in signatures_test\n",
    "        score_pos_agg = cos_sim(signatures_pos_agg, test)\n",
    "        score_neg_agg = cos_sim(signatures_neg_agg, test)\n",
    "        push!(scores, score_pos_agg/score_neg_agg) # > 1 then pos, < 1 then neg\n",
    "        push!(scores_pos, score_pos_agg)\n",
    "    end\n",
    "end\n",
    "\n",
    "# results pos vs. neg\n",
    "results = DataFrame(locus=loci_nr, rbps=rbp_nr, scores=scores, label=labels)\n",
    "CSV.write(results_dir*\"/results_HDC_grouped10CV_\"*data_suffix*\".csv\", results)\n",
    "\n",
    "# results pos only\n",
    "results = DataFrame(locus=loci_nr, rbps=rbp_nr, scores=scores_pos, label=labels)\n",
    "CSV.write(results_dir*\"/results_HDCpos_grouped10CV_\"*data_suffix*\".csv\", results)\n",
    "\n",
    "# examine scores\n",
    "histogram(scores, xlabel=\"score\", ylabel=\"count\")\n",
    "sum(scores .< 1)/length(scores)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "226146c2",
   "metadata": {},
   "source": [
    "function construct_relevant_matrix(preds_matrix, label_matrix)\n",
    "    \"\"\"\n",
    "    This function constructs a matrix of relevant predictions\n",
    "    (i.e., true positives) from the prediction matrix and label matrix.\n",
    "    \"\"\"\n",
    "    @assert size(preds_matrix) == size(label_matrix)\n",
    "    relevant_matrix = zeros(Int64, size(preds_matrix)[1], size(preds_matrix)[2])\n",
    "    for i in 1:size(preds_matrix)[1]\n",
    "        for j in 1:size(preds_matrix)[2]\n",
    "            if (preds_matrix[i,j] == label_matrix[i,j]) & (label_matrix[i,j] == 1)\n",
    "                relevant_matrix[i,j] = 1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return relevant_matrix\n",
    "end"
   ]
  },
  {
   "cell_type": "raw",
   "id": "085a9cd7",
   "metadata": {},
   "source": [
    "function mrr_from_scores_OLD(score_matrix, label_matrix)\n",
    "    \"\"\"\n",
    "    OLD version of the function is a wrapper for the previous functions that computes\n",
    "    the MRR starting from a matrix of (prediction) scores, a threshold above\n",
    "    which to consider an interaction positive and a label matrix.\n",
    "    \"\"\"\n",
    "    # construct the prediction matrix\n",
    "    preds_matrix = convert(Matrix{Int64}, score_matrix .> threshold)\n",
    "\n",
    "    # construct the relevant matrix\n",
    "    replace!(label_matrix, missing => 0)\n",
    "    rel_matrix = construct_relevant_matrix(preds_matrix, label_matrix)\n",
    "\n",
    "    # sort the matrix per row and compute\n",
    "    sorted_matrix = sort_relevant_matrix(rel_matrix, score_matrix)\n",
    "    \n",
    "    return mean_reciprocal_rank(sorted_matrix)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
